{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Using Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* [Setup Azure Arc-enabled Machine Learning Training and Inferencing on AKS on Azure Stack HCI](https://github.com/msftcoderdjw/AML-Kubernetes/blob/jiadu/aks-hci/docs/AKS-HCI/AML-ARC-Compute.md)\n",
    "\n",
    "* [Setup NFS Server on Azure Stack HCI and Use your Data and run managed Machine Learning Experiments On-Premises](https://github.com/msftcoderdjw/AML-Kubernetes/blob/jiadu/aks-hci/docs/AKS-HCI/Train-AzureArc.md)\n",
    "\n",
    "* Make sure the NFS Server is mounted on the notebook execution machine. In this notebook, it will upload the training data to NFS Server.\n",
    "\n",
    "*    Last but not least, you need to be able to run a Notebook. (azureml-core, azureml-opendatasets, numpy, matplotlib, requests are required)\n",
    "\n",
    "   If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize AzureML workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace,  ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download mnist data\n",
    "\n",
    "Perform pip install azureml-opendatasets to get the open dataset package, use this function to download mnist data later. This allows you to avoid download the data again when you run this notebook multiple times. The actual download time may take 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.opendatasets import MNIST\n",
    "import os\n",
    "\n",
    "def download_mnist_data():\n",
    "    data_folder = os.path.join(os.getcwd(), 'mnist_data')\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "    mnist_file_dataset = MNIST.get_file_dataset()\n",
    "    path = mnist_file_dataset.download(data_folder, overwrite=True)\n",
    "    downloaded_folder = os.path.dirname(path[0])\n",
    "    print(\"downloaded to\", downloaded_folder)\n",
    "    \n",
    "    return downloaded_folder\n",
    "\n",
    "downloaded_folder = download_mnist_data()\n",
    "downloaded_folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the mnist data to NFS server\n",
    "\n",
    "The above download_mnist_data() function will download four files t10k-images-idx3-ubyte.gz, t10k-labels-idx1-ubyte.gz, train-images-idx3-ubyte.gz and train-labels-idx1-ubyte.gz to downloaded_folder.  Your next step is to copy these files to NFS server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs_mount_path = \"<NFS Mount Point on notebook execution machine>\"\n",
    "\n",
    "import os, shutil\n",
    "mnist_dir = os.path.join(nfs_mount_path, 'mnist')\n",
    "shutil.rmtree(mnist_dir, ignore_errors=True)\n",
    "os.makedirs(mnist_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(downloaded_folder):\n",
    "    filepath = os.path.join(downloaded_folder, filename)\n",
    "    destpath = os.path.join(mnist_dir, filename)\n",
    "    print(f\"Copying files from {filepath} to {destpath}\")\n",
    "    shutil.copyfile(filepath, destpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup compute target\n",
    "\n",
    "Find the Arc K8S Resource Id and replace the resource id below.\n",
    "\n",
    "e.g. /subscriptions/`<subscriptionId>`/resourceGroups/`<resourceGroupName>`/providers/Microsoft.Kubernetes/connectedClusters/`<clusterName>`\n",
    "\n",
    "Using 'kubectl create ns aml' to create a namespace in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import KubernetesCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for your Azure Arc-enabled Kubernetes compute\n",
    "amlarc_compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"<ArcComputeName>\")\n",
    "\n",
    "# resource ID for your Azure Arc-enabled Kubernetes cluster\n",
    "resource_id = \"/subscriptions/<subscriptionId>/resourceGroups/<resourceGroupName>/providers/Microsoft.Kubernetes/connectedClusters/<clusterName>\"\n",
    "\n",
    "if amlarc_compute_name in ws.compute_targets:\n",
    "   amlarc_compute = ws.compute_targets[amlarc_compute_name]\n",
    "   if amlarc_compute and type(amlarc_compute) is KubernetesCompute:\n",
    "      print(\"found compute target: \" + amlarc_compute_name)\n",
    "else:\n",
    "   print(\"creating new compute target...\")\n",
    "   ns = \"aml\"\n",
    "    \n",
    "   instance_types = {\n",
    "    \"defaultInstanceType\": {\n",
    "      \"nodeSelector\": None,\n",
    "      \"resources\": {\n",
    "        \"requests\": {\n",
    "          \"cpu\": \"1\",\n",
    "          \"memory\": \"4Gi\",\n",
    "          \"nvidia.com/gpu\": 0\n",
    "        },\n",
    "        \"limits\": {\n",
    "          \"cpu\": \"1\",\n",
    "          \"memory\": \"4Gi\",\n",
    "          \"nvidia.com/gpu\": 0\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "   amlarc_attach_configuration = KubernetesCompute.attach_configuration(resource_id = resource_id, namespace = ns, default_instance_type=\"defaultInstanceType\", instance_types = instance_types)\n",
    " \n",
    "   amlarc_compute = ComputeTarget.attach(ws, amlarc_compute_name, amlarc_attach_configuration)\n",
    "\n",
    " \n",
    "   amlarc_compute.wait_for_completion(show_output=True)\n",
    "    \n",
    "   # For a more detailed view of current KubernetesCompute status, use get_status()\n",
    "   print(amlarc_compute.get_status().serialize())\n",
    "\n",
    "print(f\"compute target id in endpoint yaml: azureml:{amlarc_compute.name}, instance type name in endpoint yaml: {amlarc_compute.default_instance_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import KubernetesCompute\n",
    "\n",
    "attach_name = amlarc_compute_name\n",
    "arcK_target = KubernetesCompute(ws, attach_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the training job and submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'mnist-demo'\n",
    "\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized environment\n",
    "\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "# to install required packages\n",
    "env = Environment('tutorial-env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults'], conda_packages = ['scikit-learn==0.22.1'])\n",
    "\n",
    "env.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "The training takes about 15 mins with vm size comparable  to Standard_DS3_v2, <MountPathOnTrainingPod> is the same as the mountPath defined in mount-config.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "data_folder = \"/nfs_share\"+\"/mnist\" # training data are saved to <mountPoint>/mnist (have to use / as the path separator)\n",
    "\n",
    "args = ['--data-folder', data_folder, '--regularization', 0.5]\n",
    "script_folder =  \"mnist_script\"\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='train.py', \n",
    "                      arguments=args,\n",
    "                      compute_target=arcK_target,\n",
    "                      environment=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job\n",
    "\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=src)\n",
    "run.wait_for_completion(show_output=True)  # specify True for a verbose log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model\n",
    "\n",
    "Register the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='sklearn_mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register model\n",
    "model = run.register_model(model_name=model_name,\n",
    "                           model_path='outputs/sklearn_mnist_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model named \"sklearn_mnist\" should be registered in your AzureML workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "model = Model(ws, model_name)\n",
    "model_id = f\"azureml:{model.name}:{model.version}\"\n",
    "print(f\"Get {model.name}, latest version {model.version}, id in endpoint.yml: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and score a machine learning model by using a managed online endpoint\n",
    "\n",
    "AZ CLI only now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = '<sklearn-mnist endpoint name>'\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "prefix = Path(os.getcwd())\n",
    "endpoint_file = str(prefix.joinpath(\"endpoint.yml\"))\n",
    "print(f\"Using Endpoint file: {endpoint_file}, please replace <modelId> (e.g. azureml:sklearn_mnist:1), <instanceTypeName> (e.g. defaultInstanceType) and <computeTargetName> (e.g. azureml:amlarc-compute) according above output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')\n",
    "helpers.run(f\"az ml endpoint create -n {endpoint} -f {endpoint_file} -w {ws.name} -g {ws.resource_group}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with inputs\n",
    "\n",
    "Here you may use the image from test asset. The first 30 images and its labels are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_script.utils import load_data\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'mnist_data')\n",
    "\n",
    "X_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\n",
    "y_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\n",
    "\n",
    "# show first 30 figures\n",
    "\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "# for i in np.random.permutation(X_test.shape[0])[:sample_size]:\n",
    "for i in range(30):\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x = 10, y = -10, s = y_test[i], fontsize = 18)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap = plt.cm.Greys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get score_uri and access_token from AZ CLI (Currently only AZ CLI supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get score_url and access_token from AZ CLI\n",
    "import helpers\n",
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()\n",
    "cmd = f\"az ml endpoint show -n {endpoint} -w {ws.name} -g {ws.resource_group}\"\n",
    "properties = helpers.run(cmd, return_output=True, no_output=True)\n",
    "\n",
    "cmd = f\"az ml endpoint get-credentials -n {endpoint} -w {ws.name} -g {ws.resource_group}\"\n",
    "credentials = helpers.run(cmd, return_output=True, no_output=True)\n",
    "\n",
    "print(f\"Got endpoint and credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the second image: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "prop_response = json.loads(properties.replace(os.linesep,\"\"))\n",
    "score_uri = prop_response[\"scoring_uri\"]\n",
    "\n",
    "cred_response = json.loads(credentials.replace(os.linesep, \"\"))\n",
    "access_token = cred_response[\"accessToken\"]\n",
    "\n",
    "import requests\n",
    "# second number should be 2\n",
    "test = json.dumps({\"data\": X_test.tolist()[1:2]})\n",
    "headers = {'Content-Type': 'application/json', 'Authorization': f\"Bearer {access_token}\"}\n",
    "r = requests.post(score_uri, data=test, headers=headers)\n",
    "print(f\"predictions: {r.content}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc402497f0168b24575e2ffafe64cd34c507b9a7fab971a93b09782ae565c5c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
