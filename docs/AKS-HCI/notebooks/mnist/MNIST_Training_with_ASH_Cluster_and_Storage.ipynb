{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Using Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "*    [ A Kubernetes cluster deployed on Azure Stack Hub, connected to Azure through ARC](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/AML-ARC-Compute.md).\n",
    "     \n",
    "\n",
    "*     [Datastore setup in Azure Machine Learning workspace backed up by Azure Stack Hub storage account](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md) \n",
    "\n",
    "\n",
    "*      Last but not least, you need to be able to run a Notebook. \n",
    "\n",
    "   If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize AzureML workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace,  ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download mnist data\n",
    "\n",
    "Perform pip install azureml-opendatasets to get the open dataset package, use this function to download mnist data later. This allows you to avoid download the data again when you run this notebook multiple times. The actual download time may take 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.opendatasets import MNIST\n",
    "import os\n",
    "\n",
    "def download_mnist_data():\n",
    "    data_folder = os.path.join(os.getcwd(), 'mnist_data')\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "    mnist_file_dataset = MNIST.get_file_dataset()\n",
    "    path = mnist_file_dataset.download(data_folder, overwrite=True)\n",
    "    downloaded_folder = os.path.dirname(path[0])\n",
    "    print(\"downloaded to\", downloaded_folder)\n",
    "    \n",
    "    return downloaded_folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "The above download_mnist_data() function will download four files  t10k-images-idx3-ubyte.gz, t10k-labels-idx1-ubyte.gz, train-images-idx3-ubyte.gz and train-labels-idx1-ubyte.gz to downloaded_folder.  Your next step is to upload these files to datastore of the workspace, and then registered as dataset in the workspace. \n",
    "\n",
    "\"datastore_name\" is the name of the datastore you setup in [this step](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md).\n",
    "\n",
    "Upload and dataset registration take less than 1 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "\n",
    "dataset_name = \"mnist_ash_o\"\n",
    "datastore_name = \"<NAME_OF_ASH_HOSTED_AML_DATASTORE>\"\n",
    "\n",
    "if dataset_name not  in ws.datasets:\n",
    "    downloaded_folder = download_mnist_data()\n",
    "    datastore =  Datastore.get(ws, datastore_name)\n",
    "    \n",
    "    src_dir, target_path =downloaded_folder, 'mnistdataash'\n",
    "    datastore.upload(src_dir, target_path)\n",
    "\n",
    "    # register data uploaded as AML dataset\n",
    "    datastore_paths = [(datastore, target_path)]\n",
    "    mnist_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "    mnist_ds.register(ws, dataset_name, \"mnist data from http://yann.lecun.com/exdb/mnist/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup compute target\n",
    "\n",
    "Find the attach name for the Arc enabled  Azure Stack Hub kubernetes cluster in your AzureML workspace to create a ComputeTarget:\n",
    "\n",
    "attach_name is the attached name for your ASH cluster you setup in [this step](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/AML-ARC-Compute.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import KubernetesCompute\n",
    "\n",
    "attach_name = \"<NAME_OF_AML_ATTACHED_COMPUTE_OF_YOUR_ASH_CLUSTER>\"\n",
    "arcK_target = KubernetesCompute(ws, attach_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the training job and submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'mnist-demo'\n",
    "\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# to install required packages\n",
    "env = Environment('tutorial-env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults'], conda_packages = ['scikit-learn==0.22.1'])\n",
    "\n",
    "env.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "The training takes about 15 mins with vm size comparable  to Standard_DS3_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "args = ['--data-folder', ws.datasets[dataset_name].as_mount(), '--regularization', 0.5]\n",
    "script_folder =  \"mnist_script\"\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='train.py', \n",
    "                      arguments=args,\n",
    "                      compute_target=arcK_target,\n",
    "                      environment=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job\n",
    "\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=src)\n",
    "run.wait_for_completion(show_output=True)  # specify True for a verbose log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model\n",
    "\n",
    "Register the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register model\n",
    "model_name='sklearn_mnist'\n",
    "model = run.register_model(model_name=model_name,\n",
    "                           model_path='outputs/sklearn_mnist_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model named \"sklearn_mnist\" should be registered in your AzureML workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the registered model\n",
    "\n",
    "To test the trained model, you can create (or use existing) a AKS cluster for serving the model using AML deployment.\n",
    "\n",
    "Note: You will test the model in azure cloud because AzureML inferencing on Azure Stack Hub in currently unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Workspace, Model, ComputeTarget\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision the AKS Cluster\n",
    "\n",
    "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it. It may take 5 mins to create a new AKS cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'aks-service-2'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    is_new_compute  = False\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                    name = aks_name, \n",
    "                                    provisioning_configuration = prov_config)\n",
    "    is_new_compute  = True\n",
    "    \n",
    "print(\"using compute target: \", aks_target.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "model = Model(ws, model_name)\n",
    "\n",
    "env = Environment('tutorial-env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults'], conda_packages = ['scikit-learn==0.22.1'])\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)\n",
    "deploy_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                       name='sklearn-mnist-svc', \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_target=aks_target,\n",
    "                       deployment_config=deploy_config,\n",
    "                       overwrite=True)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with inputs\n",
    "\n",
    "Here you may use the first image (saved as digit_7.jpg) from test set, as show here:\n",
    "![fishy](digit_7.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_script.utils import load_data\n",
    "import os\n",
    "import glob\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'mnist_data')\n",
    "\n",
    "X_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\n",
    "y_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\n",
    "\n",
    "import json\n",
    "test = json.dumps({\"data\": X_test.tolist()[:1]})\n",
    "test = bytes(test, encoding='utf8')\n",
    "\n",
    "# get predicted digits:\n",
    "\n",
    "predictions = service.run(input_data=test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get 7 as predicted digit for the first image in test set ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the newly created cluster\n",
    "\n",
    "Note: This is important if you wish to avoid the cost of this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_new_compute:\n",
    "    aks_target.delete()\n",
    "    service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "1. Learn how to [distributed training with pytorch](../distributed-cifar10/distributed-pytorch-cifar10.ipynb)\n",
    "2. Learn how to [distributed training with tensorflow](../distributed-cifar10/distributed-tf2-cifar10.ipynb)\n",
    "3. Learn Pipeline Steps with [Object Segmentation](../object-segmentation-on-azure-stack/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytouchEnv",
   "language": "python",
   "name": "pytouchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}