{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Segmenation with PyTorch Using Transfer Learning\n",
    "\n",
    "For this tutorial, we will fine tune a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model in the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/). It contains 170 images with 345 instances of pedestrians, and we will use it  to train an instance segmentation model on a custom dataset defined as PennFudanDataset in aml_src/obj_segment_step_training.py. You can learn more details at\n",
    "[here](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "\n",
    "\n",
    "You will use [Azure Machine Learning Pipelines](https://aka.ms/aml-pipelines) to define two pipeline steps: a data process step which split data into training and testing, and training step which trains and evaluates the model.  The trained model then registered to your AML workspace.\n",
    "\n",
    "\n",
    "After the model is registered, you then deploy the model for testing using Azure Kubernetes Cluster (AKS).\n",
    "\n",
    "This notebook  uses ASH storage and ASH cluster (ARC compute) for training, please make sure the following prerequisites are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* [Setup Azure Arc-enabled Machine Learning Training and Inferencing on AKS on Azure Stack HCI](https://github.com/Azure/AML-Kubernetes/tree/master/docs/AKS-HCI/AML-ARC-Compute.md)\n",
    "\n",
    "* [Setup NFS Server on Azure Stack HCI and Use your Data and run managed Machine Learning Experiments On-Premises](https://github.com/Azure/AML-Kubernetes/tree/master/docs/AKS-HCI/Train-AzureArc.md)\n",
    "\n",
    "\n",
    "* Last but not least, you need to be able to run a Notebook. \n",
    "\n",
    "  If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace,Environment, Experiment, Datastore\n",
    "\n",
    "from azureml.pipeline.core import Pipeline, StepSequence\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.runconfig import RunConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AzureML workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: amlarc-sample-test-ws-eastus\n",
      "Azure region: eastus\n",
      "Subscription id: 86204643-5a96-427b-b6bb-b35b2bd6e6ce\n",
      "Resource group: AKS-HCI2\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup compute target\n",
    "\n",
    "Find the attach name for the Arc enabled AKS-HCI cluster in your AzureML workspace to create a ComputeTarget:\n",
    "\n",
    "attach_name is the attached name for your ASH cluster you setup in [this step](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/AML-ARC-Compute.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class KubernetesCompute: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute target id in endpoint yaml: azureml:arc-compute3, instance type name in endpoint yaml: defaultInstanceType\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import KubernetesCompute\n",
    "\n",
    "attach_name = \"arc-compute3\"\n",
    "arcK_target = KubernetesCompute(ws, attach_name)\n",
    "print(f\"compute target id in endpoint yaml: azureml:{arcK_target.name}, instance type name in endpoint yaml: {arcK_target.default_instance_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset to NFS server (Optional)\n",
    "\n",
    "After downloading and extracting the zip file from [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/) to your local machine, make sure you will have the following folder structure:\n",
    "\n",
    "<pre>\n",
    "PennFudanPed/\n",
    "  PedMasks/\n",
    "    FudanPed00001_mask.png\n",
    "    FudanPed00002_mask.png\n",
    "    FudanPed00003_mask.png\n",
    "    FudanPed00004_mask.png\n",
    "    ...\n",
    "  PNGImages/\n",
    "    FudanPed00001.png\n",
    "    FudanPed00002.png\n",
    "    FudanPed00003.png\n",
    "    FudanPed00004.png\n",
    "</pre>\n",
    "\n",
    "Here PennFudanPed is a sub-folder directly under working folder of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs_mount_path = \"<NFS Mount Point on notebook execution machine>\"\n",
    "downloaded_folder = os.path.join(os.getcwd(), 'PennFudanPed')\n",
    "\n",
    "import os, shutil\n",
    "penn_dir = os.path.join(nfs_mount_path, 'PennFudanPed')\n",
    "shutil.rmtree(penn_dir, ignore_errors=True)\n",
    "\n",
    "def copyFiles(source_folder, dest_folder):\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    for filename in os.listdir(source_folder):\n",
    "        filepath = os.path.join(source_folder, filename)\n",
    "        destpath = os.path.join(dest_folder, filename)\n",
    "        if os.path.isdir(filepath):\n",
    "            copyFiles(filepath, destpath)\n",
    "        else:\n",
    "            print(f\"Copying files from {filepath} to {destpath}\")\n",
    "            shutil.copyfile(filepath, destpath)\n",
    "\n",
    "copyFiles(downloaded_folder, penn_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training-test split data process step\n",
    "\n",
    "For this pipeline run, you will use two pipeline steps.  The first step is to split dataset into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_folder: /nfs_share/zjfrh_XgaIgt80\n"
     ]
    }
   ],
   "source": [
    "# create run_config first\n",
    "# data_folder = \"<MountPathOnTrainingPod>\"+\"/PennFudanPed\"\n",
    "data_folder = \"/nfs_share\"+\"/PennFudanPed\"\n",
    "\n",
    "env = Environment.from_dockerfile(\n",
    "        name='pytorch-obj-seg',\n",
    "        dockerfile='./aml_src/Dockerfile.gpu',\n",
    "        conda_specification='./aml_src/conda-env.yaml')\n",
    "\n",
    "aml_run_config = RunConfiguration()\n",
    "aml_run_config.target = arcK_target\n",
    "aml_run_config.environment = env\n",
    "\n",
    "source_directory = './aml_src'\n",
    "\n",
    "# add a data process step\n",
    "import helpers\n",
    "\n",
    "output_folder = \"/nfs_share\" + \"/\" + helpers.randFolderName()\n",
    "print(f\"output_folder: {output_folder}\")\n",
    "\n",
    "train_split_data = output_folder + \"/\" + \"train_split_data\"\n",
    "test_split_data = output_folder + \"/\" + \"test_split_data\"\n",
    "\n",
    "split_step = PythonScriptStep(\n",
    "    name=\"Train Test Split\",\n",
    "    script_name=\"obj_segment_step_data_process.py\",\n",
    "    arguments=[\"--data-path\", data_folder,\n",
    "               \"--train-split\", train_split_data, \"--test-split\", test_split_data,\n",
    "               \"--test-size\", 50],\n",
    "    compute_target=arcK_target,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=source_directory,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(\n",
    "        name=\"training_step\",\n",
    "        script_name=\"obj_segment_step_training.py\",\n",
    "        arguments=[\n",
    "            \"--train-split\", train_split_data, \"--test-split\", test_split_data,\n",
    "            '--epochs', 1,  # 80\n",
    "        ],\n",
    "\n",
    "        compute_target=arcK_target,\n",
    "        runconfig=aml_run_config,\n",
    "        source_directory=source_directory,\n",
    "        allow_reuse=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create experiment and submit pipeline run\n",
    "\n",
    "The split step takes about 8 mins. Training step takes about 25 mins per epoch for  vm comparable to Standard_DS3_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n",
      "Created step Train Test Split [ad8f9295][6423f3c8-914f-4b08-b251-b6cc140f7c3f], (This step will run and generate new outputs)\n",
      "Created step training_step [79be3675][82e3e45f-18cc-4b37-b4ce-b31290ee5647], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 79a8bcff-e170-4546-bc1f-31a9befac9e8\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/79a8bcff-e170-4546-bc1f-31a9befac9e8?wsid=/subscriptions/86204643-5a96-427b-b6bb-b35b2bd6e6ce/resourcegroups/AKS-HCI2/workspaces/amlarc-sample-test-ws-eastus&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "PipelineRunId: 79a8bcff-e170-4546-bc1f-31a9befac9e8\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/79a8bcff-e170-4546-bc1f-31a9befac9e8?wsid=/subscriptions/86204643-5a96-427b-b6bb-b35b2bd6e6ce/resourcegroups/AKS-HCI2/workspaces/amlarc-sample-test-ws-eastus&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "PipelineRun Status: Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': '79a8bcff-e170-4546-bc1f-31a9befac9e8', 'status': 'Completed', 'startTimeUtc': '2021-11-24T11:59:21.308527Z', 'endTimeUtc': '2021-11-24T12:28:55.297011Z', 'services': {}, 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'False', 'azureml.pipelineComponent': 'pipelinerun'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://amlarcsamplete7068317793.blob.core.windows.net/azureml/ExperimentRun/dcid.79a8bcff-e170-4546-bc1f-31a9befac9e8/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=As89UaMfJxTPPUJQ5WsBB8h%2BWk9yLkeXDosfRE7OO6o%3D&skoid=46817ba8-5583-4d9d-a507-647580e5f28a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2021-11-24T05%3A16%3A18Z&ske=2021-11-25T13%3A26%3A18Z&sks=b&skv=2019-07-07&st=2021-11-24T12%3A18%3A59Z&se=2021-11-24T20%3A28%3A59Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://amlarcsamplete7068317793.blob.core.windows.net/azureml/ExperimentRun/dcid.79a8bcff-e170-4546-bc1f-31a9befac9e8/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=oJLkjFc1iItwJ841UGRgEytQkfw1dhJe2UOJP%2BmF05k%3D&skoid=46817ba8-5583-4d9d-a507-647580e5f28a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2021-11-24T05%3A16%3A18Z&ske=2021-11-25T13%3A26%3A18Z&sks=b&skv=2019-07-07&st=2021-11-24T12%3A18%3A59Z&se=2021-11-24T20%3A28%3A59Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://amlarcsamplete7068317793.blob.core.windows.net/azureml/ExperimentRun/dcid.79a8bcff-e170-4546-bc1f-31a9befac9e8/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=R7dBThM4YvU4OnOvr%2BeBeT9zPbREdrDiizjDC9lTsdE%3D&skoid=46817ba8-5583-4d9d-a507-647580e5f28a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2021-11-24T05%3A16%3A18Z&ske=2021-11-25T13%3A26%3A18Z&sks=b&skv=2019-07-07&st=2021-11-24T12%3A18%3A59Z&se=2021-11-24T20%3A28%3A59Z&sp=r'}, 'submittedBy': 'Jiawei Du'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = 'obj_seg_seq_step'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "step_sequence = StepSequence(steps=[split_step, train_step])\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=step_sequence)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n",
    "pipeline_run.wait_for_completion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model\n",
    "\n",
    "Register the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_run = pipeline_run.find_step_run(train_step.name)[0]\n",
    "\n",
    "model_name = 'obj_seg_model_aml' \n",
    "train_step_run.register_model(model_name=model_name, model_path='outputs/obj_segmentation.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "model = Model(ws, model_name)\n",
    "model_id = f\"azureml:{model.name}:{model.version}\"\n",
    "print(f\"Get {model.name}, latest version {model.version}, id in deployment.yml: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model named \"obj_seg_model_aml\" should be registered in your AzureML workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the registered model\n",
    "\n",
    "To test the trained model, you can use AKS-HCI cluster for serving the model using AML deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint = '<pytorch-obj-seg endpoint name>'\n",
    "endpoint = 'pytorch-obj-seg-jiadu'\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "prefix = Path(os.getcwd())\n",
    "endpoint_file = str(prefix.joinpath(\"endpoint.yml\"))\n",
    "deployment_file = str(prefix.joinpath(\"deployment.yml\"))\n",
    "print(f\"Using Endpoint file: {endpoint_file}, Deployment file: {deployment_file} please replace <modelId> (e.g. azureml:obj_seg_model_aml:1), <instanceTypeName> (e.g. defaultInstanceType) and <computeTargetName> (e.g. azureml:amlarc-compute) according above output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to **replace the properties in deployment.yml**, including,\n",
    "* `<modelId>`: example value: azureml:obj_seg_model_aml:1\n",
    "* `<instanceTypeName>`: example value: defaultInstanceType\n",
    "\n",
    "Need to **replace the properties in endpoint.yml**, including,\n",
    "* `<computeTargetName>`: example value: azureml:amlarc-compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.run(f\"az ml online-endpoint create -n {endpoint} -f {endpoint_file} -w {ws.name} -g {ws.resource_group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.run(f\"az ml online-endpoint show -n {endpoint} -w {ws.name} -g {ws.resource_group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.run(f\"az ml online-deployment create -n blue --endpoint {endpoint} -f {deployment_file} -w {ws.name} -g {ws.resource_group} --all-traffic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with inputs\n",
    "\n",
    "For testing purpose, you may take the first image FudanPed00001.png as example. This image looks like this ![fishy](FudanPed00001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get score_url and access_token from AZ CLI\n",
    "import helpers\n",
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()\n",
    "cmd = f\"az ml online-endpoint show -n {endpoint} -w {ws.name} -g {ws.resource_group}\"\n",
    "properties = helpers.run(cmd, return_output=True, no_output=True)\n",
    "\n",
    "cmd = f\"az ml online-endpoint get-credentials -n {endpoint} -w {ws.name} -g {ws.resource_group}\"\n",
    "credentials = helpers.run(cmd, return_output=True, no_output=True)\n",
    "\n",
    "print(f\"Got endpoint and credentials.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "prop_response = json.loads(properties.replace(os.linesep,\"\"))\n",
    "score_uri = prop_response[\"scoring_uri\"]\n",
    "\n",
    "cred_response = json.loads(credentials.replace(os.linesep, \"\"))\n",
    "access_token = cred_response[\"accessToken\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "image_paths = [\"FudanPed00001.png\"]\n",
    "image_np_list = []\n",
    "for image_path in image_paths:\n",
    "    img = Image.open(image_path)\n",
    "    img_rgb = img.convert(\"RGB\")\n",
    "    img_tensor = F.to_tensor(img_rgb)\n",
    "    img_np = img_tensor.numpy()\n",
    "    image_np_list.append(img_np.tolist())\n",
    "\n",
    "inputs = json.dumps({\"instances\": image_np_list})\n",
    "\n",
    "import requests\n",
    "headers = {'Content-Type': 'application/json', 'Authorization': f\"Bearer {access_token}\"}\n",
    "r = requests.post(score_uri, data=inputs, headers=headers)\n",
    "predicts = r.json()[\"predictions\"]\n",
    "\n",
    "import numpy as np\n",
    "for instance_pred in predicts:\n",
    "    print(\"labels\", instance_pred[\"labels\"])\n",
    "    print(\"boxes\", instance_pred[\"boxes\"])\n",
    "    print(\"scores\", instance_pred[\"scores\"])\n",
    "    \n",
    "    image_data = instance_pred[\"masks\"]\n",
    "    img_np = np.array(image_data)\n",
    "    output_mask = Image.fromarray(img_np)\n",
    "    output_mask.show() #show the image\n",
    "    output_mask.save(\"predict_mask.png\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc402497f0168b24575e2ffafe64cd34c507b9a7fab971a93b09782ae565c5c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
