{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Image Classification with PyTorch\n",
    "\n",
    "In this tutorial, you will train a PyTorch model on the [CIFAR10](http://www.cs.toronto.edu/~kriz/cifar.html) dataset using distributed training with PyTorch's `DistributedDataParallel` module across a AKS-HCI CPU cluster. The training dataset are stored on the on-premise NFS Server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* [Setup Azure Arc-enabled Machine Learning Training and Inferencing on AKS on Azure Stack HCI](https://github.com/Azure/AML-Kubernetes/tree/master/docs/AKS-HCI/AML-ARC-Compute.md)\n",
    "\n",
    "* [Setup NFS Server on Azure Stack HCI and Use your Data and run managed Machine Learning Experiments On-Premises](https://github.com/Azure/AML-Kubernetes/tree/master/docs/AKS-HCI/Train-AzureArc.md)\n",
    "\n",
    "* Make sure the NFS Server is mounted on the notebook execution machine. In this notebook, it will upload the training data to NFS Server.\n",
    "\n",
    "* Last but not least, you need to be able to run a Notebook. (azureml-core, azureml-opendatasets, numpy, matplotlib, requests are required)\n",
    "\n",
    "   If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize AzureML workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace,  ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download cifar10 data\n",
    "\n",
    "Use this function to download cifar10 data later. This function allows you to avoid download the data again when you run this notebook multiple times. The actual download time may take 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "def download_cifar10_data():\n",
    "    \n",
    "    path = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "    downloaded_folder = os.path.join(os.getcwd(), 'cifar10-data')\n",
    "    os.makedirs(downloaded_folder, exist_ok=True) # download data to 'cifar10-data' folder\n",
    "\n",
    "    data = requests.get(path, allow_redirects=True).content\n",
    "    with open(os.path.join(downloaded_folder, path.split('/')[-1]), 'wb') as f:\n",
    "        f.write(data)\n",
    "        \n",
    "    return downloaded_folder\n",
    "\n",
    "downloaded_folder = download_cifar10_data()\n",
    "downloaded_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the cifar10 data to NFS server (Optional)\n",
    "\n",
    "The above download_cifar10_data() function will download cifar-10-python.tar.gz to downloaded_folder.  Your next step is to copy these files to NFS server.\n",
    "Run run this step, you need to make sure the NFS path is mounted on the notebook execution machine. Replace `<NFS Mount Point on notebook execution machine>` with the correct path.\n",
    "\n",
    "This step is optional, you can use other ways, such as directly downloading training data on NFS server to prepare the cifar10 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs_mount_path = \"<NFS Mount Point on notebook execution machine>\"\n",
    "\n",
    "import os, shutil\n",
    "cifar10_dir = os.path.join(nfs_mount_path, 'cifar10')\n",
    "shutil.rmtree(cifar10_dir, ignore_errors=True)\n",
    "os.makedirs(cifar10_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(downloaded_folder):\n",
    "    filepath = os.path.join(downloaded_folder, filename)\n",
    "    destpath = os.path.join(cifar10_dir, filename)\n",
    "    print(f\"Copying files from {filepath} to {destpath}\")\n",
    "    shutil.copyfile(filepath, destpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup compute target\n",
    "\n",
    "Find the attach name for the Arc enabled AKS-HCI in your AzureML workspace.\n",
    "\n",
    "attach_name is the attached name for your ASH cluster you setup in [this step](https://github.com/Azure/AML-Kubernetes/tree/master/docs/AKS-HCI/AML-ARC-Compute.md#attach-your-azure-arc-enabled-cluster-to-your-azure-machine-learning-workspace-as-a-compute-target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import KubernetesCompute\n",
    "\n",
    "# attach_name = \"arc-compute2\"\n",
    "attach_name = \"aksarc-compute\"\n",
    "arcK_target = KubernetesCompute(ws, attach_name)\n",
    "print(\"using compute target: \", arcK_target.name)\n",
    "print(f\"compute target id in endpoint yaml: azureml:{arcK_target.name}, instance type name in endpoint yaml: {arcK_target.default_instance_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the training job and submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch-cifar-distr'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "\n",
    "pytorch_env = Environment.from_conda_specification(name = 'pytorch-1.6-cpu', file_path = 'pytorch-script/conda_dependencies.yml')\n",
    "\n",
    "# Specify a CPU base image\n",
    "pytorch_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04'\n",
    "docker_config = DockerConfiguration(use_docker=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job: torch.distributed with GLOO backend\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on.\n",
    "\n",
    "In order to run a distributed PyTorch job with **torch.distributed** using the GLOO backend, create a `PyTorchConfiguration` and pass it to the `distributed_job_config` parameter of the ScriptRunConfig constructor. Specify `communication_backend='Gloo'` in the PyTorchConfiguration. The below code will configure node_count = 2. These is the number of worker nodes. The number of  distributed jobs will be 3 if one master node is used.  GLOO backend which is recommended backend for communications between CPUs.\n",
    "\n",
    "Tthe script for distributed training of CIFAR10 is already provided for you at `pytorch-script/cifar_dist_main.py`. In practice, you should be able to take any custom PyTorch training script as is and run it with Azure ML without having to modify your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "from azureml.core import Dataset\n",
    "import os\n",
    "\n",
    "data_folder = \"/nfs_share\" + \"/cifar10\"\n",
    "args = [\n",
    "        '--data-folder', data_folder,\n",
    "        '--dist-backend', 'gloo',\n",
    "       '--epochs', 1 #20\n",
    "           ]\n",
    "\n",
    "distributed_job_config=PyTorchConfiguration(communication_backend='Gloo', node_count=2) #configuring AML pytorch config\n",
    "\n",
    "project_folder = \"pytorch-script\"\n",
    "run_script = \"cifar_dist_main.py\"\n",
    "src = ScriptRunConfig(\n",
    "                     source_directory=project_folder,\n",
    "                      script=run_script,\n",
    "                      arguments=args,\n",
    "                      compute_target=arcK_target,\n",
    "                      environment=pytorch_env,\n",
    "                      distributed_job_config=distributed_job_config,\n",
    "                      docker_runtime_config=docker_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)\n",
    "run.wait_for_completion(show_output=True) # this provides a verbose log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model\n",
    "\n",
    "Register the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_model_name = 'cifar10torch'\n",
    "model = run.register_model(model_name=register_model_name, model_path='outputs/cifar10torch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model named \"cifar10torch\" should be registered in your AzureML workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "model = Model(ws, register_model_name)\n",
    "model_id = f\"azureml:{model.name}:{model.version}\"\n",
    "print(f\"Get {model.name}, latest version {model.version}, id in endpoint.yml: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and score a machine learning model by using a managed online endpoint\n",
    "\n",
    "Currently, amlarc inference is only supported with Azure Machine Learning CI (v2). Please follow [doc](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-cli?view=azure-devops#prerequisites) to configure the prerequisites. This notebook will invoke Azure Machine Learning CI (v2) directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint = '<cifar10torch endpoint name>'\n",
    "endpoint = 'cifar10torch-jiadu'\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "prefix = Path(os.getcwd())\n",
    "endpoint_file = str(prefix.joinpath(\"pytorch_endpoint.yml\"))\n",
    "print(f\"Using Endpoint file: {endpoint_file}, please replace <modelId> (e.g. azureml:cifar10torch:1), <instanceTypeName> (e.g. defaultInstanceType) and <computeTargetName> (e.g. azureml:amlarc-compute) according above output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to **replace the properties in endpoint.yml**, including,\n",
    "* `<modelId>`: example value: azureml:sklearn_mnist:1\n",
    "* `<instanceTypeName>`: example value: defaultInstanceType\n",
    "* `<computeTargetName>`: example value: azureml:amlarc-compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')\n",
    "helpers.run(f\"az ml endpoint create -n {endpoint} -f {endpoint_file} -w {ws.name} -g {ws.resource_group}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get score_uri and access_token from AZ CLI (Currently only Azure Machine Learning CLI (v2) supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get score_url and access_token from AZ CLI\n",
    "import helpers\n",
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()\n",
    "cmd = f\"az ml endpoint show -n {endpoint} -w {ws.name} -g {ws.resource_group}\"\n",
    "properties = helpers.run(cmd, return_output=True, no_output=True)\n",
    "\n",
    "cmd = f\"az ml endpoint get-credentials -n {endpoint} -w {ws.name} -g {ws.resource_group}\"\n",
    "credentials = helpers.run(cmd, return_output=True, no_output=True)\n",
    "\n",
    "print(f\"Got endpoint and credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with inputs\n",
    "\n",
    "For testing purpose, first 5 images (cat, ship, ship, plane, frog) is extracted (saved at test_imgs/test_image_*.jpg).\n",
    "\n",
    "After some data process, the images are converted to json as input for the trained model. The outputs are logits for each class per image. \n",
    "\n",
    "The pre-processed request json is saved to test_request_pytorch/cifar_test_input_img_*.json. \n",
    "\n",
    "test_request_pytorch/cifar_test_input_img_first_5_pytorch.json is the request json including first 5 images.\n",
    "\n",
    "Below cell visualizes the first 5 images using matplotlib, and also extracted the expected labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'cifar10-data')\n",
    "extracted_folder = os.path.join(data_folder, \"extracted\")\n",
    "os.makedirs(extracted_folder, exist_ok=True)\n",
    "\n",
    "cifar_gz = glob.glob(os.path.join(data_folder,\"**/cifar-10-python.tar.gz\"), recursive=True)[0]\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "    return dict\n",
    "\n",
    "with tarfile.open(cifar_gz, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extracted_folder)\n",
    "    tar.close()\n",
    "\n",
    "train_batches = glob.glob(os.path.join(extracted_folder, \"**/data_batch_*\"), recursive=True)\n",
    "test_batch = glob.glob(os.path.join(extracted_folder, \"**/test_batch*\"), recursive=True)[0]\n",
    "meta_file = glob.glob(os.path.join(extracted_folder, \"**/batches.meta\"), recursive=True)[0]\n",
    "\n",
    "# data_batch = unpickle(train_batches[0])\n",
    "data_batch = unpickle(test_batch)\n",
    "meta_data = unpickle(meta_file)\n",
    "\n",
    "# take the images data from batch data\n",
    "images = data_batch['data']\n",
    "# reshape and transpose the images\n",
    "images = images.reshape(len(images),3,32,32).transpose(0,2,3,1)\n",
    "# take labels of the images \n",
    "labels = data_batch['labels']\n",
    "# label names of the images\n",
    "label_names = meta_data['label_names']\n",
    "\n",
    "# dispaly random images\n",
    "# define row and column of figure\n",
    "rows, columns = 1, 5\n",
    "# take random image idex id\n",
    "# imageId = np.random.randint(0, len(images), rows * columns)\n",
    "imageId = np.arange(0, rows * columns)\n",
    "# take images for above random image ids\n",
    "images = images[imageId]\n",
    "# take labels for these images only\n",
    "labels = [labels[i] for i in imageId]\n",
    "\n",
    "# define figure\n",
    "fig=plt.figure(figsize=(10, 10))\n",
    "# visualize these random images\n",
    "for i in range(1, columns*rows +1):\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(images[i-1])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"{}\"\n",
    "          .format(label_names[labels[i-1]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use test_request_pytorch/cifar_test_input_img_first_5_pytorch.json to test first 5 images to the trained model. \n",
    "The response shows the prediction of each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "prop_response = json.loads(properties.replace(os.linesep,\"\"))\n",
    "score_uri = prop_response[\"scoring_uri\"]\n",
    "\n",
    "cred_response = json.loads(credentials.replace(os.linesep, \"\"))\n",
    "access_token = cred_response[\"accessToken\"]\n",
    "\n",
    "def parse_predicts(predicts):\n",
    "    import numpy as np\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "            'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    np_predicts = np.array(predicts)\n",
    "    pred_indexes = np.argmax(np_predicts, 1)\n",
    "\n",
    "    predict_labels = [classes[i] for i in pred_indexes]\n",
    "    return predict_labels\n",
    "\n",
    "import requests\n",
    "total = 5\n",
    "match = 0\n",
    "testClasses = [label_names[i] for i in labels]\n",
    "\n",
    "with open(os.path.join(\"test_request_pytorch\", f\"cifar_test_input_img_first_{total}_pytorch.json\"), \"r\") as fp:\n",
    "    inputs_json = json.load(fp)\n",
    "\n",
    "inputs = json.dumps(inputs_json)\n",
    "\n",
    "headers = {'Content-Type': 'application/json', 'Authorization': f\"Bearer {access_token}\"}\n",
    "r = requests.post(score_uri, data=inputs, headers=headers)\n",
    "\n",
    "predicts = r.json()[\"predicts\"]\n",
    "\n",
    "predict_labels = parse_predicts(predicts)\n",
    "\n",
    "for i in range(0, total):\n",
    "    test_label = testClasses[i]\n",
    "    predict_label = predict_labels[i]\n",
    "    if test_label == predict_label:\n",
    "        match = match + 1\n",
    "    print(f\"Image {i}, tag_label: {test_label}, predict_label: {predict_label}\")\n",
    "\n",
    "acc = match / total / 1.0\n",
    "print(f\"Tested {total} images, accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can easily get the predictions of labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your model, you may or may not get the correct label which is \"cat\"."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "ninhu"
   }
  ],
  "category": "training",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "MNIST"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "PyTorch"
  ],
  "friendly_name": "Distributed training with PyTorch",
  "index_order": 1,
  "interpreter": {
   "hash": "fc402497f0168b24575e2ffafe64cd34c507b9a7fab971a93b09782ae565c5c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "tags": [
   "None"
  ],
  "task": "Train a model using distributed training via Nccl/Gloo"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
