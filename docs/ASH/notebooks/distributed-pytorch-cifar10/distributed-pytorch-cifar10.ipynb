{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed PyTorch with DistributedDataParallel\n",
    "In this tutorial, you will train a PyTorch model on the [CIFAR10](http://www.cs.toronto.edu/~kriz/cifar.html) dataset using distributed training with PyTorch's `DistributedDataParallel` module across a Azure Stack Hub CPU Kubernetes cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Please see Prerequisite part of [this notebook](../pipeline/nyc-taxi-data-regression-model-building.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace,  ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Here we download cifar10 dataset from [cifar10-data](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz). The downloaded data is then registered as dataset in a data store of the workspace. \n",
    "\n",
    "To set up datastore using an azure stack hub storage account, please refer to [Train_azure_arc](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md#create-and-configure-azure-stack-hubs-storage-account). \n",
    "\n",
    "To register the dataset manually, please refer to this [video](https://msit.microsoftstream.com/video/51f7a3ff-0400-b9eb-2703-f1eb38bc6232)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import requests\n",
    "from azureml.core import Dataset\n",
    "\n",
    "from azureml.exceptions import UserErrorException\n",
    "dataset_name = 'CIFAR-10'\n",
    "datastore_name = \"ashstore\"\n",
    "\n",
    "if dataset_name not  in ws.datasets:\n",
    "    path = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        os.makedirs(os.path.join(tmpdir, 'cifar-10'))\n",
    "\n",
    "        data = requests.get(path, allow_redirects=True).content\n",
    "        with open(os.path.join(tmpdir, 'cifar-10', path.split('/')[-1]), 'wb') as f:\n",
    "            f.write(data)\n",
    "    \n",
    "        \n",
    "        ds = Dataset.File.upload_directory(tmpdir, ws.datastores.get(datastore_name), overwrite=True)\n",
    "        ds.register(ws, dataset_name, 'CIFAR-10 images from https://www.cs.toronto.edu/~kriz/cifar.html')\n",
    "        \n",
    "dataset_ash = ws.datasets[dataset_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or attach existing ArcKubernetesCompute\n",
    "\n",
    "The attaching code here depends  python package azureml-contrib-k8s which current is in private preview. Install private preview branch of AzureML SDK by running following command (private preview):\n",
    "\n",
    "<pre>\n",
    "pip install --disable-pip-version-check --extra-index-url https://azuremlsdktestpypi.azureedge.net/azureml-contrib-k8s-preview/D58E86006C65 azureml-contrib-k8s\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.core.compute.arckubernetescompute import ArcKubernetesCompute\n",
    "\n",
    "resource_id = \"/subscriptions/6b736da6-3246-44dd-a0b8-b5e95484633d/resourceGroups/AML-stack-val/providers/Microsoft.Kubernetes/connectedClusters/kub-orlando-Test\"\n",
    "\n",
    "attach_config = ArcKubernetesCompute.attach_configuration(\n",
    "    resource_id= resource_id,\n",
    ")\n",
    "\n",
    "try:\n",
    "    attach_name = \"peymanarc\"\n",
    "    arcK_target_result = ArcKubernetesCompute.attach(ws, attach_name, attach_config)\n",
    "    arcK_target_result.wait_for_completion(show_output=True)\n",
    "    print('arc attach  success')\n",
    "except ComputeTargetException as e:\n",
    "    print(e)\n",
    "    print('arc attach  failed')\n",
    "\n",
    "attach_name = \"ds3v2\" #\"nc6\"\n",
    "arcK_target = ws.compute_targets[attach_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute\n",
    "Now that we have the ArcKubernetesCompute ready to go, let's run our distributed training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script and any additional files your training script depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './pytorch-distr'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training script\n",
    "Now you will need to create your training script. In this tutorial, the script for distributed training of CIFAR10 is already provided for you at `cifar_dist_main.py`. In practice, you should be able to take any custom PyTorch training script as is and run it with Azure ML without having to modify your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your script is ready, copy the training script `cifar_dist_main.py` into the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "run_script = 'cifar_dist_main.py'\n",
    "shutil.copy(run_script, project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this distributed PyTorch tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch-cifar-distr'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n",
    "\n",
    "Define a conda environment YAML file with your training script dependencies and create an Azure ML environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conda_dependencies.yml\n",
    "\n",
    "channels:\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - torch==1.6.0\n",
    "  - torchvision==0.7.0\n",
    "  - future==0.17.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "pytorch_env = Environment.from_conda_specification(name = 'pytorch-1.6-cpu', file_path = './conda_dependencies.yml')\n",
    "\n",
    "# Specify a CPU base image\n",
    "\n",
    "pytorch_env.docker.enabled = True\n",
    "pytorch_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job: torch.distributed with GLOO backend\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on.\n",
    "\n",
    "In order to run a distributed PyTorch job with **torch.distributed** using the GLOO backend, create a `PyTorchConfiguration` and pass it to the `distributed_job_config` parameter of the ScriptRunConfig constructor. Specify `communication_backend='Gloo'` in the PyTorchConfiguration. The below code will configure node_count = 2. These is the number of worker nodes. The number of  distributed jobs will be 3 if one master node is used.  GLOO backend which is recommended backend for communications between CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "from azureml.core import Dataset\n",
    "import os\n",
    "\n",
    "dataset_ash = Dataset.get_by_name(ws, name=dataset_name)\n",
    "args = [\n",
    "        '--data-folder', dataset_ash.as_mount(),\n",
    "        '--dist-backend', 'gloo',\n",
    "       '--epochs', 1\n",
    "           ]\n",
    "\n",
    "distributed_job_config=PyTorchConfiguration(communication_backend='Gloo', node_count=3) #configuring AML pytorch config\n",
    "\n",
    "\n",
    "src = ScriptRunConfig(\n",
    "                     source_directory=project_folder,\n",
    "                      script=run_script,\n",
    "                      arguments=args,\n",
    "                      compute_target=arcK_target,\n",
    "                      environment=pytorch_env,\n",
    "                      distributed_job_config=distributed_job_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)\n",
    "run.wait_for_completion(show_output=True) # this provides a verbose log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the model is saved at path \"outputs/001\"\n",
    "# register the model\n",
    "model_name = 'cifar10torchcpupt'\n",
    "model = run.register_model(model_name=model_name, model_path='outputs/cifar10torch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model named \"cifar10torch\" should be registered in your AML workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Registered Model\n",
    "\n",
    "To test the trained model, you can create (or use existing) a AKS cluster for serving the model using AML deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Workspace, Model, ComputeTarget\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'aks-service-2'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    is_new_compute  = False\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws,\n",
    "                                    name = aks_name,\n",
    "                                    provisioning_configuration = prov_config)\n",
    "    is_new_compute  = True\n",
    "\n",
    "if aks_target.get_status() != \"Succeeded\":\n",
    "    aks_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.from_conda_specification(name='pytorch-cifar', file_path='./conda_dependencies.yml')\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
    "deploy_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "model = ws.models[\"cifar10torchcpupt\"]\n",
    "service_name = 'cifartorchservice4'\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=deploy_config,\n",
    "                       deployment_target=aks_target,\n",
    "                       overwrite=True)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cifar_input.json\", \"r\") as fp:\n",
    "    inputs_json = json.load(fp)\n",
    "    \n",
    "inputs = json.dumps(inputs_json)\n",
    "resp = service.run(inputs)\n",
    "predicts = resp[\"predicts\"]\n",
    "predicts_str = json.dumps(predicts)\n",
    "\n",
    "print(predicts_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the newly created cluster\n",
    "\n",
    "Note: This is important if you wish to avoid the cost of this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_new_compute:\n",
    "    aks_target.delete()\n",
    "    service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Learn how to [download model then upload to Azure Storage blobs](../AML-model-download-upload.ipynb)\n",
    "2. Learn how to [inference using KFServing with model in Azure Storage Blobs](https://aka.ms/kfas)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "ninhu"
   }
  ],
  "category": "training",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "MNIST"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "PyTorch"
  ],
  "friendly_name": "Distributed training with PyTorch",
  "index_order": 1,
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "pythonproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "tags": [
   "None"
  ],
  "task": "Train a model using distributed training via Nccl/Gloo"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
