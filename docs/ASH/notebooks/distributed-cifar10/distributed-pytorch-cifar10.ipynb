{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/ml-frameworks/pytorch/distributed-pytorch-with-horovod/distributed-pytorch-with-horovod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed PyTorch with DistributedDataParallel\n",
    "In this tutorial, you will train a PyTorch model on the [CIFAR10](http://www.cs.toronto.edu/~kriz/cifar.html) dataset using distributed training with PyTorch's `DistributedDataParallel` module across a Azure Stack Hub CPU Kubernetes cluster. The training dataset are stored on Azure Machine Learning datastore backed up by Azure Stack Hub Storage account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "*     A Kubernetes cluster deployed on Azure Stack Hub, connected to Azure through ARC.\n",
    "     \n",
    "   For details on how to deploy kubernetes cluster on Azure Stack Hub and enabling ARC connection to Azure, please follow [this guide](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/AML-ARC-Compute.md)\n",
    "  \n",
    "\n",
    "*     Datastore setup in Azure Machine Learning workspace backed up by Azure Stack Hub storage account.\n",
    "\n",
    "   [This document](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md) is a detailed guide on how to create Azure Machine Learning workspace, create a  Azure Stack Hub Storage account, and setup datastore in AML workspace backed by ASH storage account.\n",
    "\n",
    "\n",
    "*      Last but not least, you need to be able to run a Notebook. \n",
    "\n",
    "   If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace,  ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Here we download cifar10 dataset from [cifar10-data](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz). For your convenience, the data is downloaded to cifar10-data folder in this repo. The downloaded data is then uploaded to datastore of the workspace, and finally registered as dataset in the workspace. \n",
    "\n",
    "To set up datastore using an azure stack hub storage account, please refer to [Train_azure_arc](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md). To register the dataset manually, please refer to this [video](https://msit.microsoftstream.com/video/51f7a3ff-0400-b9eb-2703-f1eb38bc6232)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import requests\n",
    "from azureml.core import Dataset, Datastore\n",
    "\n",
    "from azureml.exceptions import UserErrorException\n",
    "dataset_name = 'cifar10'\n",
    "datastore_name = \"ashstore\"\n",
    "\n",
    "if dataset_name not  in ws.datasets:\n",
    "    datastore =  Datastore.get(ws, datastore_name)\n",
    "    \n",
    "    src_dir, target_path = 'cifar10-data', 'cifar10-data-ash'\n",
    "    \n",
    "    # upload data from local to AML datastore:\n",
    "    datastore.upload(src_dir, target_path)\n",
    "\n",
    "    # register data uploaded as AML dataset:\n",
    "    datastore_paths = [(datastore, target_path)]\n",
    "    cifar_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "    cifar_ds.register(ws, dataset_name, \"CIFAR-10 images from https://www.cs.toronto.edu/~kriz/cifar.html\")\n",
    "        \n",
    "dataset_ash = ws.datasets[dataset_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or attach existing ArcKubernetesCompute\n",
    "\n",
    "The attaching code here depends  python package azureml-contrib-k8s which current is in private preview. Install private preview branch of AzureML SDK by running following command (private preview):\n",
    "\n",
    "<pre>\n",
    "pip install --disable-pip-version-check --extra-index-url https://azuremlsdktestpypi.azureedge.net/azureml-contrib-k8s-preview/D58E86006C65 azureml-contrib-k8s\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.core.compute.arckubernetescompute import ArcKubernetesCompute\n",
    "\n",
    "resource_id = \"/subscriptions/6b736da6-3246-44dd-a0b8-b5e95484633d/resourceGroups/AML-stack-val/providers/Microsoft.Kubernetes/connectedClusters/kub-orlando-Test\"\n",
    "\n",
    "attach_config = ArcKubernetesCompute.attach_configuration(\n",
    "    resource_id= resource_id,\n",
    ")\n",
    "\n",
    "try:\n",
    "    attach_name = \"peymanarc\"\n",
    "    arcK_target_result = ArcKubernetesCompute.attach(ws, attach_name, attach_config)\n",
    "    arcK_target_result.wait_for_completion(show_output=True)\n",
    "    print('arc attach  success')\n",
    "except ComputeTargetException as e:\n",
    "    print(e)\n",
    "    print('arc attach  failed')\n",
    "\n",
    "attach_name = \"nc6\"#\"ds3v2\" \n",
    "arcK_target = ws.compute_targets[attach_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job and Submit a run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch-cifar-distr'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "pytorch_env = Environment.from_conda_specification(name = 'pytorch-1.6-cpu', file_path = 'pytorch-script/conda_dependencies.yml')\n",
    "\n",
    "# Specify a CPU base image\n",
    "\n",
    "pytorch_env.docker.enabled = True\n",
    "pytorch_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job: torch.distributed with GLOO backend\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on.\n",
    "\n",
    "In order to run a distributed PyTorch job with **torch.distributed** using the GLOO backend, create a `PyTorchConfiguration` and pass it to the `distributed_job_config` parameter of the ScriptRunConfig constructor. Specify `communication_backend='Gloo'` in the PyTorchConfiguration. The below code will configure node_count = 2. These is the number of worker nodes. The number of  distributed jobs will be 3 if one master node is used.  GLOO backend which is recommended backend for communications between CPUs.\n",
    "\n",
    "Tthe script for distributed training of CIFAR10 is already provided for you at `pytorch-script/cifar_dist_main.py`. In practice, you should be able to take any custom PyTorch training script as is and run it with Azure ML without having to modify your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "from azureml.core import Dataset\n",
    "import os\n",
    "\n",
    "dataset_ash = Dataset.get_by_name(ws, name=dataset_name)\n",
    "args = [\n",
    "        '--data-folder', dataset_ash.as_mount(),\n",
    "        '--dist-backend', 'gloo',\n",
    "       '--epochs', 20\n",
    "           ]\n",
    "\n",
    "distributed_job_config=PyTorchConfiguration(communication_backend='Gloo', node_count=2) #configuring AML pytorch config\n",
    "\n",
    "project_folder = \"pytorch-script\"\n",
    "run_script = \"cifar_dist_main.py\"\n",
    "src = ScriptRunConfig(\n",
    "                     source_directory=project_folder,\n",
    "                      script=run_script,\n",
    "                      arguments=args,\n",
    "                      compute_target=arcK_target,\n",
    "                      environment=pytorch_env,\n",
    "                      distributed_job_config=distributed_job_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)\n",
    "run.wait_for_completion(show_output=True) # this provides a verbose log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the model\n",
    "register_model_name = 'cifar10torch'\n",
    "model = run.register_model(model_name=register_model_name, model_path='outputs/cifar10torch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model named \"cifar10torch\" should be registered in your AML workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Registered Model\n",
    "\n",
    "To test the trained model, you can create (or use existing) a AKS cluster for serving the model using AML deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Workspace, Model, ComputeTarget\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'aks-service-2'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    is_new_compute  = False\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws,\n",
    "                                    name = aks_name,\n",
    "                                    provisioning_configuration = prov_config)\n",
    "    is_new_compute  = True\n",
    "\n",
    "if aks_target.get_status() != \"Succeeded\":\n",
    "    aks_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.from_conda_specification(name='pytorch-cifar', file_path='pytorch-script/conda_dependencies.yml')\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score_pytorch.py', environment=env)\n",
    "deploy_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "#register_model_name = \"cifar10torch80\"\n",
    "model = ws.models[register_model_name]\n",
    "service_name = 'cifartorchservice4'\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=deploy_config,\n",
    "                       deployment_target=aks_target,\n",
    "                       overwrite=True)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Inputs:\n",
    "\n",
    "For testing purpose, first five images from test batch are extracted. Let's take a look these pictures: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_files = [\"test_img_0_cat.jpg\", \"test_img_1_ship.jpg\",\"test_img_2_ship.jpg\",\"test_img_3_plane.jpg\",\"test_img_4_frog.jpg\"]\n",
    "\n",
    "for img_file in img_files:\n",
    "    image = Image.open(img_file)\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some data process, these five images are converted to json as input for the trained model. The outputs are logits for each class per image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cifar_test_input_pytorch.json\", \"r\") as fp:\n",
    "    inputs_json = json.load(fp)\n",
    "    \n",
    "inputs = json.dumps(inputs_json)\n",
    "resp = service.run(inputs)\n",
    "predicts = resp[\"predicts\"]\n",
    "print(predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can easily get the predictions of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "np_predicts = np.array(predicts)\n",
    "pred_indexes = np.argmax(np_predicts, 1)\n",
    "\n",
    "predict_labels = [classes[i] for i in pred_indexes]\n",
    "print(predict_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the newly created cluster\n",
    "\n",
    "Note: This is important if you wish to avoid the cost of this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_new_compute:\n",
    "    aks_target.delete()\n",
    "    service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Learn how to [download model then upload to Azure Storage blobs](../AML-model-download-upload.ipynb)\n",
    "2. Learn how to [inference using KFServing with model in Azure Storage Blobs](https://aka.ms/kfas)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "ninhu"
   }
  ],
  "category": "training",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "MNIST"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "PyTorch"
  ],
  "friendly_name": "Distributed training with PyTorch",
  "index_order": 1,
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "pythonproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "tags": [
   "None"
  ],
  "task": "Train a model using distributed training via Nccl/Gloo"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
