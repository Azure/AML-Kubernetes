{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed PyTorch with DistributedDataParallel\n",
    "In this tutorial, you will train a PyTorch model on the [CIFAR10](http://www.cs.toronto.edu/~kriz/cifar.html) dataset using distributed training with PyTorch's `DistributedDataParallel` module across a Azure Stack Hub CPU Kubernetes cluster. The training dataset are stored on Azure Machine Learning datastore backed up by Azure Stack Hub Storage account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "*     A Kubernetes cluster deployed on Azure Stack Hub, connected to Azure through ARC.\n",
    "     \n",
    "   For details on how to deploy kubernetes cluster on Azure Stack Hub and enabling ARC connection to Azure, please follow [this guide](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/AML-ARC-Compute.md)\n",
    "  \n",
    "\n",
    "*     Datastore setup in Azure Machine Learning workspace backed up by Azure Stack Hub storage account.\n",
    "\n",
    "   [This document](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md) is a detailed guide on how to create Azure Machine Learning workspace, create a  Azure Stack Hub Storage account, and setup datastore in AML workspace backed by ASH storage account.\n",
    "\n",
    "\n",
    "*      Last but not least, you need to be able to run a Notebook. \n",
    "\n",
    "   If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.22.0\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: sl-ash2-mal\n",
      "Azure region: eastus\n",
      "Subscription id: 6b736da6-3246-44dd-a0b8-b5e95484633d\n",
      "Resource group: sl-ash2\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.workspace import Workspace,  ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "You may download cifar10 dataset from [cifar10-data](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz). Create folder \"cifar10-data\" under working directory of this notebook, then  copy \"cifar-10-python.tar.gz\" to folder \"cifar10-data\". The following cell will upload \"cifar-10-python.tar.gz\" to datastore of the workspace, and finally registered as dataset in the workspace. \n",
    "\n",
    "Upload and dataset registration take about 3 mins.\n",
    "\n",
    "To set up datastore using an azure stack hub storage account, please refer to [Train_azure_arc](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md). To register the dataset manually, please refer to this [video](https://msit.microsoftstream.com/video/51f7a3ff-0400-b9eb-2703-f1eb38bc6232)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Datastore, Dataset\n",
    "\n",
    "dataset_name = 'cifar10'\n",
    "datastore_name = \"ashstore\"\n",
    "\n",
    "if dataset_name not  in ws.datasets:\n",
    "    datastore =  Datastore.get(ws, datastore_name)\n",
    "    \n",
    "    src_dir, target_path = 'cifar10-data', 'cifar10-data-ash' #assuming cifar-10-python.tar.gz is in folder cifar10-data\n",
    "    \n",
    "    # upload data from local to AML datastore:\n",
    "    datastore.upload(src_dir, target_path)\n",
    "\n",
    "    # register data uploaded as AML dataset:\n",
    "    datastore_paths = [(datastore, target_path)]\n",
    "    cifar_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "    cifar_ds.register(ws, dataset_name, \"CIFAR-10 images from https://www.cs.toronto.edu/~kriz/cifar.html\")\n",
    "        \n",
    "dataset_ash = ws.datasets[dataset_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or attach existing ArcKubernetesCompute\n",
    "\n",
    "The attaching code here depends  python package azureml-contrib-k8s which current is in private preview. Install private preview branch of AzureML SDK by running following command (private preview):\n",
    "\n",
    "<pre>\n",
    "pip install --disable-pip-version-check --extra-index-url https://azuremlsdktestpypi.azureedge.net/azureml-contrib-k8s-preview/D58E86006C65 azureml-contrib-k8s\n",
    "</pre>\n",
    "\n",
    "Attaching ASH cluster the first time may take 7 minutes. It will be much faster after first attachment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "arc attach  success\n"
     ]
    }
   ],
   "source": [
    "from azureml.contrib.core.compute.arckubernetescompute import ArcKubernetesCompute\n",
    "from azureml.core import ComputeTarget\n",
    "\n",
    "resource_id = \"/subscriptions/6b736da6-3246-44dd-a0b8-b5e95484633d/resourceGroups/sl-ash2/providers/Microsoft.Kubernetes/connectedClusters/sl-d2-o-arc\"\n",
    "\n",
    "attach_config = ArcKubernetesCompute.attach_configuration(\n",
    "    resource_id= resource_id,\n",
    ")\n",
    "\n",
    "try:\n",
    "    attach_name = \"sl-d2-o-arc\"\n",
    "    arcK_target_result = ArcKubernetesCompute.attach(ws, attach_name, attach_config)\n",
    "    arcK_target_result.wait_for_completion(show_output=True)\n",
    "    print('arc attach  success')\n",
    "except ComputeTargetException as e:\n",
    "    print(e)\n",
    "    print('arc attach  failed')\n",
    "\n",
    "attach_name = \"nc6\"\n",
    "arcK_target = ComputeTarget(ws, attach_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the training job and Submit a run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch-cifar-distr'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "pytorch_env = Environment.from_conda_specification(name = 'pytorch-1.6-cpu', file_path = 'pytorch-script/conda_dependencies.yml')\n",
    "\n",
    "# Specify a CPU base image\n",
    "pytorch_env.docker.enabled = True\n",
    "pytorch_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job: torch.distributed with GLOO backend\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on.\n",
    "\n",
    "In order to run a distributed PyTorch job with **torch.distributed** using the GLOO backend, create a `PyTorchConfiguration` and pass it to the `distributed_job_config` parameter of the ScriptRunConfig constructor. Specify `communication_backend='Gloo'` in the PyTorchConfiguration. The below code will configure node_count = 2. These is the number of worker nodes. The number of  distributed jobs will be 3 if one master node is used.  GLOO backend which is recommended backend for communications between CPUs.\n",
    "\n",
    "Tthe script for distributed training of CIFAR10 is already provided for you at `pytorch-script/cifar_dist_main.py`. In practice, you should be able to take any custom PyTorch training script as is and run it with Azure ML without having to modify your code.\n",
    "\n",
    "With node_count=2, training for one epoch may take 20 mins with vm size comparable to Standard_DS3_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "from azureml.core import Dataset\n",
    "import os\n",
    "\n",
    "dataset_ash = Dataset.get_by_name(ws, name=dataset_name)\n",
    "args = [\n",
    "        '--data-folder', dataset_ash.as_mount(),\n",
    "        '--dist-backend', 'gloo',\n",
    "       '--epochs', 1 #20\n",
    "           ]\n",
    "\n",
    "distributed_job_config=PyTorchConfiguration(communication_backend='Gloo', node_count=2) #configuring AML pytorch config\n",
    "\n",
    "project_folder = \"pytorch-script\"\n",
    "run_script = \"cifar_dist_main.py\"\n",
    "src = ScriptRunConfig(\n",
    "                     source_directory=project_folder,\n",
    "                      script=run_script,\n",
    "                      arguments=args,\n",
    "                      compute_target=arcK_target,\n",
    "                      environment=pytorch_env,\n",
    "                      distributed_job_config=distributed_job_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: pytorch-cifar-distr_1613693036_9d4025fd\n",
      "Web View: https://ml.azure.com/experiments/pytorch-cifar-distr/runs/pytorch-cifar-distr_1613693036_9d4025fd?wsid=/subscriptions/6b736da6-3246-44dd-a0b8-b5e95484633d/resourcegroups/sl-ash2/workspaces/sl-ash2-mal\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d.txt\n",
      "========================================================================================================================\n",
      "\n",
      "2021-02-19T00:08:06Z Starting output-watcher...\n",
      "2021-02-19T00:08:06Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "[2021-02-19T00:08:31.345165] Entering job preparation.\n",
      "[2021-02-19T00:08:31.893023] Starting job preparation.\n",
      "[2021-02-19T00:08:31.893062] Extracting the control code.\n",
      "[2021-02-19T00:08:31.936700] fetching and extracting the control code on master node.\n",
      "[2021-02-19T00:08:31.936728] Starting extract_project.\n",
      "[2021-02-19T00:08:31.936760] Starting to extract zip file.\n",
      "[2021-02-19T00:08:32.693754] Finished extracting zip file.\n",
      "[2021-02-19T00:08:32.805348] Using urllib.request Python 3.0 or later\n",
      "[2021-02-19T00:08:32.805428] Start fetching snapshots.\n",
      "[2021-02-19T00:08:32.805469] Start fetching snapshot.\n",
      "[2021-02-19T00:08:32.805487] Retrieving project from snapshot: 7ffab322-f63b-4caa-90ed-c23bfd907305\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 63\n",
      "[2021-02-19T00:08:33.242799] Finished fetching snapshot.\n",
      "[2021-02-19T00:08:33.242830] Finished fetching snapshots.\n",
      "[2021-02-19T00:08:33.242846] Finished extract_project.\n",
      "[2021-02-19T00:08:33.262939] Finished fetching and extracting the control code.\n",
      "[2021-02-19T00:08:33.269151] Start run_history_prep.\n",
      "[2021-02-19T00:08:33.448086] Job preparation is complete.\n",
      "[2021-02-19T00:08:33.448132] Entering Data Context Managers in Sidecar\n",
      "[2021-02-19T00:08:33.448779] Running Sidecar prep cmd...\n",
      "[2021-02-19T00:08:33.506479] INFO azureml.sidecar.sidecar: Received task: enter_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/sl-ash2-mal/azureml/pytorch-cifar-distr_1613693036_9d4025fd/mounts/workspaceblobstore/azureml/pytorch-cifar-distr_1613693036_9d4025fd\n",
      "[2021-02-19T00:08:33.507187] INFO azureml.sidecar.sidecar: Invoking \"enter_contexts\" task with Context Managers: {\"context_managers\": [\"Dataset:context_managers.Datasets\"]}\n",
      "Enter __enter__ of DatasetContextManager\n",
      "SDK version: azureml-core==1.21.0.post2 azureml-dataprep==2.9.1. Session id: 0333008f-7d18-451f-a5ca-a637e2702dab. Run id: pytorch-cifar-distr_1613693036_9d4025fd.\n",
      "Processing 'input__f1c41f8b'.\n",
      "Processing dataset FileDataset\n",
      "{\n",
      "  \"source\": [\n",
      "    \"('ashstore', 'cifar10-data-ash')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"24d86baf-41f9-40e1-ae0d-5ae8b4c9cdfc\",\n",
      "    \"name\": \"cifar10\",\n",
      "    \"version\": 1,\n",
      "    \"description\": \"CIFAR-10 images from https://www.cs.toronto.edu/~kriz/cifar.html\",\n",
      "    \"workspace\": \"Workspace.create(name='sl-ash2-mal', subscription_id='6b736da6-3246-44dd-a0b8-b5e95484633d', resource_group='sl-ash2')\"\n",
      "  }\n",
      "}\n",
      "Mounting input__f1c41f8b to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/sl-ash2-mal/azureml/pytorch-cifar-distr_1613693036_9d4025fd/wd/tmpg5mx86tx.\n",
      "Mounted input__f1c41f8b to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/sl-ash2-mal/azureml/pytorch-cifar-distr_1613693036_9d4025fd/wd/tmpg5mx86tx as folder.\n",
      "Exit __enter__ of DatasetContextManager\n",
      "Set Dataset input__f1c41f8b's target path to /mnt/batch/tasks/shared/LS_root/jobs/sl-ash2-mal/azureml/pytorch-cifar-distr_1613693036_9d4025fd/wd/tmpg5mx86tx\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 1\n",
      "[2021-02-19T00:17:03.857592] INFO azureml.sidecar.task.enter_contexts: Entered Context Managers\n",
      "[2021-02-19T00:17:05.491728] Ran Sidecar prep cmd.\n",
      "[2021-02-19T00:17:05.491774] Running Context Managers in Sidecar complete.\n",
      "\n",
      "Streaming azureml-logs/70_driver_log_0.txt\n",
      "==========================================\n",
      "\n",
      "bash: /azureml-envs/azureml_77ae6faafb422d20b955420f1d57d91e/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "[2021-02-19T00:17:39.698757] Entering context manager injector.\n",
      "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError', 'UserExceptions:context_managers.UserExceptions'], invocation=['cifar_dist_main.py', '--data-folder', 'DatasetConsumptionConfig:input__f1c41f8b', '--dist-backend', 'gloo', '--epochs', '1'])\n",
      "Script type = None\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 109\n",
      "[2021-02-19T00:17:41.784574] Entering Run History Context Manager.\n",
      "[2021-02-19T00:17:42.694477] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/sl-ash2-mal/azureml/pytorch-cifar-distr_1613693036_9d4025fd/mounts/workspaceblobstore/azureml/pytorch-cifar-distr_1613693036_9d4025fd\n",
      "[2021-02-19T00:17:42.694677] Preparing to call script [cifar_dist_main.py] with arguments:['--data-folder', '$input__f1c41f8b', '--dist-backend', 'gloo', '--epochs', '1']\n",
      "[2021-02-19T00:17:42.694731] After variable expansion, calling script [cifar_dist_main.py] with arguments:['--data-folder', '/mnt/batch/tasks/shared/LS_root/jobs/sl-ash2-mal/azureml/pytorch-cifar-distr_1613693036_9d4025fd/wd/tmpg5mx86tx', '--dist-backend', 'gloo', '--epochs', '1']\n",
      "\n",
      "data fold /mnt/batch/tasks/shared/LS_root/jobs/sl-ash2-mal/azureml/pytorch-cifar-distr_1613693036_9d4025fd/wd/tmpg5mx86tx\n",
      "rank=  0\n",
      "world_size=  2\n",
      "==> Preparing data..\n",
      "data_folder /mnt/batch/tasks/shared/LS_root/jobs/sl-ash2-mal/azureml/pytorch-cifar-distr_1613693036_9d4025fd/wd/tmpg5mx86tx\n",
      "==> Building model..\n",
      "epoch 0\n",
      "\n",
      "Epoch: 0\n",
      "0 196 Loss: 2.341 | Acc: 7.812% (10/128)\n",
      "1 196 Loss: 2.337 | Acc: 9.375% (24/256)\n",
      "2 196 Loss: 2.308 | Acc: 12.500% (48/384)\n",
      "3 196 Loss: 2.308 | Acc: 11.133% (57/512)\n",
      "4 196 Loss: 2.309 | Acc: 11.719% (75/640)\n",
      "5 196 Loss: 2.310 | Acc: 12.500% (96/768)\n",
      "6 196 Loss: 2.290 | Acc: 14.174% (127/896)\n",
      "7 196 Loss: 2.261 | Acc: 14.746% (151/1024)\n",
      "8 196 Loss: 2.244 | Acc: 15.278% (176/1152)\n",
      "9 196 Loss: 2.237 | Acc: 16.250% (208/1280)\n",
      "10 196 Loss: 2.217 | Acc: 16.903% (238/1408)\n",
      "11 196 Loss: 2.208 | Acc: 17.383% (267/1536)\n",
      "12 196 Loss: 2.203 | Acc: 17.849% (297/1664)\n",
      "13 196 Loss: 2.191 | Acc: 18.192% (326/1792)\n",
      "14 196 Loss: 2.195 | Acc: 18.177% (349/1920)\n",
      "15 196 Loss: 2.197 | Acc: 18.213% (373/2048)\n",
      "16 196 Loss: 2.184 | Acc: 18.980% (413/2176)\n",
      "17 196 Loss: 2.174 | Acc: 19.271% (444/2304)\n",
      "18 196 Loss: 2.167 | Acc: 19.572% (476/2432)\n",
      "19 196 Loss: 2.159 | Acc: 20.117% (515/2560)\n",
      "20 196 Loss: 2.149 | Acc: 20.461% (550/2688)\n",
      "21 196 Loss: 2.145 | Acc: 20.703% (583/2816)\n",
      "22 196 Loss: 2.136 | Acc: 21.264% (626/2944)\n",
      "23 196 Loss: 2.135 | Acc: 21.322% (655/3072)\n",
      "24 196 Loss: 2.131 | Acc: 21.594% (691/3200)\n",
      "25 196 Loss: 2.127 | Acc: 21.725% (723/3328)\n",
      "26 196 Loss: 2.118 | Acc: 21.991% (760/3456)\n",
      "27 196 Loss: 2.109 | Acc: 22.182% (795/3584)\n",
      "28 196 Loss: 2.102 | Acc: 22.387% (831/3712)\n",
      "29 196 Loss: 2.093 | Acc: 22.500% (864/3840)\n",
      "30 196 Loss: 2.088 | Acc: 22.555% (895/3968)\n",
      "31 196 Loss: 2.081 | Acc: 22.778% (933/4096)\n",
      "32 196 Loss: 2.075 | Acc: 23.011% (972/4224)\n",
      "33 196 Loss: 2.071 | Acc: 23.300% (1014/4352)\n",
      "34 196 Loss: 2.070 | Acc: 23.371% (1047/4480)\n",
      "35 196 Loss: 2.064 | Acc: 23.524% (1084/4608)\n",
      "36 196 Loss: 2.056 | Acc: 23.754% (1125/4736)\n",
      "37 196 Loss: 2.052 | Acc: 23.890% (1162/4864)\n",
      "38 196 Loss: 2.047 | Acc: 24.099% (1203/4992)\n",
      "39 196 Loss: 2.048 | Acc: 23.965% (1227/5120)\n",
      "40 196 Loss: 2.045 | Acc: 24.066% (1263/5248)\n",
      "41 196 Loss: 2.038 | Acc: 24.256% (1304/5376)\n",
      "42 196 Loss: 2.033 | Acc: 24.437% (1345/5504)\n",
      "43 196 Loss: 2.026 | Acc: 24.645% (1388/5632)\n",
      "44 196 Loss: 2.022 | Acc: 24.774% (1427/5760)\n",
      "45 196 Loss: 2.016 | Acc: 24.932% (1468/5888)\n",
      "46 196 Loss: 2.008 | Acc: 25.299% (1522/6016)\n",
      "47 196 Loss: 2.005 | Acc: 25.553% (1570/6144)\n",
      "48 196 Loss: 2.001 | Acc: 25.638% (1608/6272)\n",
      "49 196 Loss: 1.993 | Acc: 25.891% (1657/6400)\n",
      "50 196 Loss: 1.989 | Acc: 26.042% (1700/6528)\n",
      "51 196 Loss: 1.986 | Acc: 26.172% (1742/6656)\n",
      "52 196 Loss: 1.985 | Acc: 26.179% (1776/6784)\n",
      "53 196 Loss: 1.979 | Acc: 26.447% (1828/6912)\n",
      "54 196 Loss: 1.976 | Acc: 26.491% (1865/7040)\n",
      "55 196 Loss: 1.971 | Acc: 26.562% (1904/7168)\n",
      "56 196 Loss: 1.967 | Acc: 26.754% (1952/7296)\n",
      "57 196 Loss: 1.964 | Acc: 26.859% (1994/7424)\n",
      "58 196 Loss: 1.964 | Acc: 26.960% (2036/7552)\n",
      "59 196 Loss: 1.961 | Acc: 27.044% (2077/7680)\n",
      "60 196 Loss: 1.956 | Acc: 27.267% (2129/7808)\n",
      "61 196 Loss: 1.953 | Acc: 27.407% (2175/7936)\n",
      "62 196 Loss: 1.950 | Acc: 27.530% (2220/8064)\n",
      "63 196 Loss: 1.946 | Acc: 27.661% (2266/8192)\n",
      "64 196 Loss: 1.944 | Acc: 27.644% (2300/8320)\n",
      "65 196 Loss: 1.943 | Acc: 27.699% (2340/8448)\n",
      "66 196 Loss: 1.939 | Acc: 27.845% (2388/8576)\n",
      "67 196 Loss: 1.936 | Acc: 27.884% (2427/8704)\n",
      "68 196 Loss: 1.933 | Acc: 27.933% (2467/8832)\n",
      "69 196 Loss: 1.931 | Acc: 27.980% (2507/8960)\n",
      "70 196 Loss: 1.925 | Acc: 28.202% (2563/9088)\n",
      "71 196 Loss: 1.922 | Acc: 28.331% (2611/9216)\n",
      "72 196 Loss: 1.919 | Acc: 28.360% (2650/9344)\n",
      "73 196 Loss: 1.916 | Acc: 28.484% (2698/9472)\n",
      "74 196 Loss: 1.914 | Acc: 28.542% (2740/9600)\n",
      "75 196 Loss: 1.911 | Acc: 28.742% (2796/9728)\n",
      "76 196 Loss: 1.908 | Acc: 28.815% (2840/9856)\n",
      "77 196 Loss: 1.904 | Acc: 28.936% (2889/9984)\n",
      "78 196 Loss: 1.901 | Acc: 29.035% (2936/10112)\n",
      "79 196 Loss: 1.899 | Acc: 29.150% (2985/10240)\n",
      "80 196 Loss: 1.898 | Acc: 29.157% (3023/10368)\n",
      "81 196 Loss: 1.894 | Acc: 29.278% (3073/10496)\n",
      "82 196 Loss: 1.892 | Acc: 29.320% (3115/10624)\n",
      "83 196 Loss: 1.889 | Acc: 29.381% (3159/10752)\n",
      "84 196 Loss: 1.888 | Acc: 29.393% (3198/10880)\n",
      "85 196 Loss: 1.886 | Acc: 29.406% (3237/11008)\n",
      "86 196 Loss: 1.884 | Acc: 29.445% (3279/11136)\n",
      "87 196 Loss: 1.883 | Acc: 29.457% (3318/11264)\n",
      "88 196 Loss: 1.881 | Acc: 29.468% (3357/11392)\n",
      "89 196 Loss: 1.878 | Acc: 29.618% (3412/11520)\n",
      "90 196 Loss: 1.877 | Acc: 29.705% (3460/11648)\n",
      "91 196 Loss: 1.875 | Acc: 29.798% (3509/11776)\n",
      "92 196 Loss: 1.873 | Acc: 29.889% (3558/11904)\n",
      "93 196 Loss: 1.873 | Acc: 29.862% (3593/12032)\n",
      "94 196 Loss: 1.871 | Acc: 29.942% (3641/12160)\n",
      "95 196 Loss: 1.869 | Acc: 29.989% (3685/12288)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 196 Loss: 1.867 | Acc: 30.034% (3729/12416)\n",
      "97 196 Loss: 1.864 | Acc: 30.118% (3778/12544)\n",
      "98 196 Loss: 1.861 | Acc: 30.169% (3823/12672)\n",
      "99 196 Loss: 1.859 | Acc: 30.227% (3869/12800)\n",
      "100 196 Loss: 1.857 | Acc: 30.306% (3918/12928)\n",
      "101 196 Loss: 1.855 | Acc: 30.323% (3959/13056)\n",
      "102 196 Loss: 1.853 | Acc: 30.438% (4013/13184)\n",
      "103 196 Loss: 1.852 | Acc: 30.476% (4057/13312)\n",
      "104 196 Loss: 1.849 | Acc: 30.618% (4115/13440)\n",
      "105 196 Loss: 1.847 | Acc: 30.727% (4169/13568)\n",
      "106 196 Loss: 1.845 | Acc: 30.834% (4223/13696)\n",
      "107 196 Loss: 1.842 | Acc: 30.961% (4280/13824)\n",
      "108 196 Loss: 1.840 | Acc: 31.064% (4334/13952)\n",
      "109 196 Loss: 1.838 | Acc: 31.179% (4390/14080)\n",
      "110 196 Loss: 1.836 | Acc: 31.194% (4432/14208)\n",
      "111 196 Loss: 1.834 | Acc: 31.250% (4480/14336)\n",
      "112 196 Loss: 1.832 | Acc: 31.305% (4528/14464)\n",
      "113 196 Loss: 1.830 | Acc: 31.380% (4579/14592)\n",
      "114 196 Loss: 1.829 | Acc: 31.393% (4621/14720)\n",
      "115 196 Loss: 1.828 | Acc: 31.472% (4673/14848)\n",
      "116 196 Loss: 1.828 | Acc: 31.544% (4724/14976)\n",
      "117 196 Loss: 1.826 | Acc: 31.614% (4775/15104)\n",
      "118 196 Loss: 1.824 | Acc: 31.729% (4833/15232)\n",
      "119 196 Loss: 1.823 | Acc: 31.803% (4885/15360)\n",
      "120 196 Loss: 1.820 | Acc: 31.909% (4942/15488)\n",
      "121 196 Loss: 1.818 | Acc: 31.954% (4990/15616)\n",
      "122 196 Loss: 1.816 | Acc: 32.069% (5049/15744)\n",
      "123 196 Loss: 1.815 | Acc: 32.145% (5102/15872)\n",
      "124 196 Loss: 1.814 | Acc: 32.175% (5148/16000)\n",
      "125 196 Loss: 1.812 | Acc: 32.230% (5198/16128)\n",
      "126 196 Loss: 1.810 | Acc: 32.283% (5248/16256)\n",
      "127 196 Loss: 1.810 | Acc: 32.312% (5294/16384)\n",
      "128 196 Loss: 1.808 | Acc: 32.370% (5345/16512)\n",
      "129 196 Loss: 1.807 | Acc: 32.410% (5393/16640)\n",
      "130 196 Loss: 1.806 | Acc: 32.449% (5441/16768)\n",
      "131 196 Loss: 1.804 | Acc: 32.552% (5500/16896)\n",
      "132 196 Loss: 1.802 | Acc: 32.619% (5553/17024)\n",
      "133 196 Loss: 1.800 | Acc: 32.684% (5606/17152)\n",
      "134 196 Loss: 1.798 | Acc: 32.778% (5664/17280)\n",
      "135 196 Loss: 1.797 | Acc: 32.807% (5711/17408)\n",
      "136 196 Loss: 1.795 | Acc: 32.870% (5764/17536)\n",
      "137 196 Loss: 1.795 | Acc: 32.880% (5808/17664)\n",
      "138 196 Loss: 1.794 | Acc: 32.914% (5856/17792)\n",
      "139 196 Loss: 1.793 | Acc: 32.991% (5912/17920)\n",
      "140 196 Loss: 1.791 | Acc: 33.034% (5962/18048)\n",
      "141 196 Loss: 1.790 | Acc: 33.082% (6013/18176)\n",
      "142 196 Loss: 1.788 | Acc: 33.151% (6068/18304)\n",
      "143 196 Loss: 1.787 | Acc: 33.209% (6121/18432)\n",
      "144 196 Loss: 1.786 | Acc: 33.314% (6183/18560)\n",
      "145 196 Loss: 1.783 | Acc: 33.401% (6242/18688)\n",
      "146 196 Loss: 1.781 | Acc: 33.477% (6299/18816)\n",
      "147 196 Loss: 1.779 | Acc: 33.515% (6349/18944)\n",
      "148 196 Loss: 1.779 | Acc: 33.583% (6405/19072)\n",
      "149 196 Loss: 1.778 | Acc: 33.661% (6463/19200)\n",
      "150 196 Loss: 1.776 | Acc: 33.759% (6525/19328)\n",
      "151 196 Loss: 1.774 | Acc: 33.856% (6587/19456)\n",
      "152 196 Loss: 1.772 | Acc: 33.915% (6642/19584)\n",
      "153 196 Loss: 1.771 | Acc: 33.969% (6696/19712)\n",
      "154 196 Loss: 1.770 | Acc: 34.032% (6752/19840)\n",
      "155 196 Loss: 1.767 | Acc: 34.140% (6817/19968)\n",
      "156 196 Loss: 1.765 | Acc: 34.241% (6881/20096)\n",
      "157 196 Loss: 1.765 | Acc: 34.286% (6934/20224)\n",
      "158 196 Loss: 1.763 | Acc: 34.360% (6993/20352)\n",
      "159 196 Loss: 1.763 | Acc: 34.419% (7049/20480)\n",
      "160 196 Loss: 1.762 | Acc: 34.462% (7102/20608)\n",
      "161 196 Loss: 1.761 | Acc: 34.500% (7154/20736)\n",
      "162 196 Loss: 1.758 | Acc: 34.567% (7212/20864)\n",
      "163 196 Loss: 1.757 | Acc: 34.594% (7262/20992)\n",
      "164 196 Loss: 1.756 | Acc: 34.631% (7314/21120)\n",
      "165 196 Loss: 1.755 | Acc: 34.653% (7363/21248)\n",
      "166 196 Loss: 1.753 | Acc: 34.707% (7419/21376)\n",
      "167 196 Loss: 1.751 | Acc: 34.784% (7480/21504)\n",
      "168 196 Loss: 1.750 | Acc: 34.805% (7529/21632)\n",
      "169 196 Loss: 1.749 | Acc: 34.881% (7590/21760)\n",
      "170 196 Loss: 1.747 | Acc: 34.905% (7640/21888)\n",
      "171 196 Loss: 1.745 | Acc: 34.979% (7701/22016)\n",
      "172 196 Loss: 1.744 | Acc: 35.052% (7762/22144)\n",
      "173 196 Loss: 1.742 | Acc: 35.143% (7827/22272)\n",
      "174 196 Loss: 1.740 | Acc: 35.232% (7892/22400)\n",
      "175 196 Loss: 1.740 | Acc: 35.241% (7939/22528)\n",
      "176 196 Loss: 1.739 | Acc: 35.284% (7994/22656)\n",
      "177 196 Loss: 1.738 | Acc: 35.393% (8064/22784)\n",
      "178 196 Loss: 1.736 | Acc: 35.444% (8121/22912)\n",
      "179 196 Loss: 1.735 | Acc: 35.495% (8178/23040)\n",
      "180 196 Loss: 1.734 | Acc: 35.523% (8230/23168)\n",
      "181 196 Loss: 1.732 | Acc: 35.564% (8285/23296)\n",
      "182 196 Loss: 1.730 | Acc: 35.651% (8351/23424)\n",
      "183 196 Loss: 1.728 | Acc: 35.700% (8408/23552)\n",
      "184 196 Loss: 1.728 | Acc: 35.718% (8458/23680)\n",
      "185 196 Loss: 1.728 | Acc: 35.681% (8495/23808)\n",
      "186 196 Loss: 1.727 | Acc: 35.716% (8549/23936)\n",
      "187 196 Loss: 1.726 | Acc: 35.784% (8611/24064)\n",
      "188 196 Loss: 1.725 | Acc: 35.818% (8665/24192)\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "[2021-02-19T00:20:24.416412] Entering job release\n",
      "[2021-02-19T00:20:25.661698] Starting job release\n",
      "[2021-02-19T00:20:25.662596] Logging experiment finalizing status in history service.\n",
      "[2021-02-19T00:20:25.662753] job release stage : upload_datastore starting...Starting the daemon thread to refresh tokens in background for process with pid = 472\n",
      "\n",
      "[2021-02-19T00:20:25.663005] job release stage : start importing azureml.history._tracking in run_history_release.\n",
      "[2021-02-19T00:20:25.663780] job release stage : copy_batchai_cached_logs starting...[2021-02-19T00:20:25.663817] job release stage : execute_job_release starting...\n",
      "\n",
      "[2021-02-19T00:20:25.665792] job release stage : copy_batchai_cached_logs completed...\n",
      "[2021-02-19T00:20:25.721656] Entering context manager injector.\n",
      "[2021-02-19T00:20:25.781246] job release stage : upload_datastore completed...\n",
      "[2021-02-19T00:20:25.835653] job release stage : send_run_telemetry starting...\n",
      "[2021-02-19T00:20:25.889054] job release stage : execute_job_release completed...\n",
      "[2021-02-19T00:20:26.014570] get vm size and vm region successfully.\n",
      "[2021-02-19T00:20:26.028711] get compute meta data successfully.\n",
      "[2021-02-19T00:20:26.414441] post artifact meta request successfully.\n",
      "[2021-02-19T00:20:26.444581] upload compute record artifact successfully.\n",
      "[2021-02-19T00:20:26.602089] job release stage : send_run_telemetry completed...\n",
      "[2021-02-19T00:20:26.602285] Running in AzureML-Sidecar, starting to exit user context managers...\n",
      "[2021-02-19T00:20:26.602368] Running Sidecar release cmd...\n",
      "[2021-02-19T00:20:26.616054] INFO azureml.sidecar.sidecar: Received task: exit_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/sl-ash2-mal/azureml/pytorch-cifar-distr_1613693036_9d4025fd/mounts/workspaceblobstore/azureml/pytorch-cifar-distr_1613693036_9d4025fd\n",
      "Enter __exit__ of DatasetContextManager\n",
      "Unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/sl-ash2-mal/azureml/pytorch-cifar-distr_1613693036_9d4025fd/wd/tmpg5mx86tx.\n",
      "Finishing unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/sl-ash2-mal/azureml/pytorch-cifar-distr_1613693036_9d4025fd/wd/tmpg5mx86tx.\n",
      "Exit __exit__ of DatasetContextManager\n",
      "[2021-02-19T00:20:26.760514] Removing absolute paths from host...\n",
      "[2021-02-19T00:20:26.900289] INFO azureml.sidecar.task.exit_contexts: Exited Context Managers\n",
      "[2021-02-19T00:20:27.188151] Ran Sidecar release cmd.\n",
      "[2021-02-19T00:20:27.188188] Job release is complete\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: pytorch-cifar-distr_1613693036_9d4025fd\n",
      "Web View: https://ml.azure.com/experiments/pytorch-cifar-distr/runs/pytorch-cifar-distr_1613693036_9d4025fd?wsid=/subscriptions/6b736da6-3246-44dd-a0b8-b5e95484633d/resourcegroups/sl-ash2/workspaces/sl-ash2-mal\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'pytorch-cifar-distr_1613693036_9d4025fd',\n",
       " 'target': 'nc6',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2021-02-19T00:08:05.413047Z',\n",
       " 'endTimeUtc': '2021-02-19T00:20:42.240971Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': 'a89a5286-de7e-4bc6-9986-8bb8ab74f679',\n",
       "  'azureml.git.repository_uri': 'git@github.com:lisongshan007/AML-Kubernetes.git',\n",
       "  'mlflow.source.git.repoURL': 'git@github.com:lisongshan007/AML-Kubernetes.git',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': '9af78d232734dace4df94ed28a5e6888ed3905e4',\n",
       "  'mlflow.source.git.commit': '9af78d232734dace4df94ed28a5e6888ed3905e4',\n",
       "  'azureml.git.dirty': 'False',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
       " 'inputDatasets': [{'dataset': {'id': '24d86baf-41f9-40e1-ae0d-5ae8b4c9cdfc'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'input__f1c41f8b', 'mechanism': 'Mount'}}],\n",
       " 'outputDatasets': [],\n",
       " 'runDefinition': {'script': 'cifar_dist_main.py',\n",
       "  'command': '',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--data-folder',\n",
       "   'DatasetConsumptionConfig:input__f1c41f8b',\n",
       "   '--dist-backend',\n",
       "   'gloo',\n",
       "   '--epochs',\n",
       "   '1'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'communicator': 'Gloo',\n",
       "  'target': 'nc6',\n",
       "  'dataReferences': {},\n",
       "  'data': {'input__f1c41f8b': {'dataLocation': {'dataset': {'id': '24d86baf-41f9-40e1-ae0d-5ae8b4c9cdfc',\n",
       "      'name': 'cifar10',\n",
       "      'version': '1'},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Mount',\n",
       "    'environmentVariableName': 'input__f1c41f8b',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'outputData': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': 2592000,\n",
       "  'nodeCount': 2,\n",
       "  'priority': None,\n",
       "  'credentialPassthrough': False,\n",
       "  'identity': None,\n",
       "  'environment': {'name': 'pytorch-1.6-cpu',\n",
       "   'version': 'Autosave_2020-12-10T03:47:24Z_55cb9ba6',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-defaults',\n",
       "        'torch==1.6.0',\n",
       "        'torchvision==0.7.0',\n",
       "        'future==0.17.1']}],\n",
       "     'name': 'azureml_77ae6faafb422d20b955420f1d57d91e'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'enableMLflowTracking': True,\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': None},\n",
       "  'aiSuperComputer': {'instanceType': None,\n",
       "   'frameworkImage': None,\n",
       "   'imageVersion': None,\n",
       "   'location': None,\n",
       "   'aiSuperComputerStorageData': None,\n",
       "   'interactive': False,\n",
       "   'scalePolicy': None},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'pyTorch': {'communicationBackend': 'gloo', 'processCount': None},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'commandReturnCodeConfig': {'returnCode': 'Zero',\n",
       "   'successfulReturnCodes': []},\n",
       "  'environmentVariables': {}},\n",
       " 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/azureml-logs/55_azureml-execution-tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d.txt?sv=2019-02-02&sr=b&sig=by0bQ5fGdhsuO058C73bm36BXoUb3QrE5DZCkW6O%2BMw%3D&st=2021-02-19T00%3A11%3A20Z&se=2021-02-19T08%3A21%3A20Z&sp=r',\n",
       "  'azureml-logs/55_azureml-execution-tvmps_b268f66e9e58536935eb2696e0558dc3cdad9f657f752c291ae0ca85f5955f66_d.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/azureml-logs/55_azureml-execution-tvmps_b268f66e9e58536935eb2696e0558dc3cdad9f657f752c291ae0ca85f5955f66_d.txt?sv=2019-02-02&sr=b&sig=f29MywJ3bE%2F2zYo8J0%2FXYDrbsc%2Bs8sWH7iiKo2klFTM%3D&st=2021-02-19T00%3A11%3A21Z&se=2021-02-19T08%3A21%3A21Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/azureml-logs/65_job_prep-tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d.txt?sv=2019-02-02&sr=b&sig=ovGWCQK1iazwkbjAittturn0IReQ4u%2Bu9M2hMrQ3yd4%3D&st=2021-02-19T00%3A11%3A21Z&se=2021-02-19T08%3A21%3A21Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_b268f66e9e58536935eb2696e0558dc3cdad9f657f752c291ae0ca85f5955f66_d.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/azureml-logs/65_job_prep-tvmps_b268f66e9e58536935eb2696e0558dc3cdad9f657f752c291ae0ca85f5955f66_d.txt?sv=2019-02-02&sr=b&sig=ICZzcHNNaVHUMOoQk2E70boDsHn7y%2FjR5OjDH%2Fp5G1I%3D&st=2021-02-19T00%3A11%3A21Z&se=2021-02-19T08%3A21%3A21Z&sp=r',\n",
       "  'azureml-logs/70_driver_log_0.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/azureml-logs/70_driver_log_0.txt?sv=2019-02-02&sr=b&sig=cNgtDyhr%2BJmxZ0y4CvE1%2BtBqx0DE%2FfeH5cH87k4Xigw%3D&st=2021-02-19T00%3A11%3A21Z&se=2021-02-19T08%3A21%3A21Z&sp=r',\n",
       "  'azureml-logs/70_driver_log_1.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/azureml-logs/70_driver_log_1.txt?sv=2019-02-02&sr=b&sig=ivYKgoVUUH9%2FZ2bhOoq1cOLYG79rRrlv6bsXw6eH7O4%3D&st=2021-02-19T00%3A11%3A21Z&se=2021-02-19T08%3A21%3A21Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/azureml-logs/75_job_post-tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d.txt?sv=2019-02-02&sr=b&sig=WvzInS0s7TZr8oPCGCDj%2Ba5W0EsZLPxuX%2BrL1ijymOg%3D&st=2021-02-19T00%3A11%3A21Z&se=2021-02-19T08%3A21%3A21Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_b268f66e9e58536935eb2696e0558dc3cdad9f657f752c291ae0ca85f5955f66_d.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/azureml-logs/75_job_post-tvmps_b268f66e9e58536935eb2696e0558dc3cdad9f657f752c291ae0ca85f5955f66_d.txt?sv=2019-02-02&sr=b&sig=LN6H8tWAkxxFWyktdceNPgUgPLJCtTUDqrOTwSsjBvA%3D&st=2021-02-19T00%3A11%3A21Z&se=2021-02-19T08%3A21%3A21Z&sp=r',\n",
       "  'azureml-logs/process_info.json': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=U80Ah%2BoDOouTmId9zmf6cooDWl5SDcPIkKW8vbdCDWI%3D&st=2021-02-19T00%3A11%3A21Z&se=2021-02-19T08%3A21%3A21Z&sp=r',\n",
       "  'azureml-logs/process_status.json': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=gYKSo5FKWwokPFpOKF80LI6sU2LIstA9vER78xGSI3k%3D&st=2021-02-19T00%3A11%3A21Z&se=2021-02-19T08%3A21%3A21Z&sp=r',\n",
       "  'logs/azureml/0_109_azureml.log': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/logs/azureml/0_109_azureml.log?sv=2019-02-02&sr=b&sig=Co2EoGkdyobtsW16GXfNGtLwsUFhBZrWqUHb3c%2B%2BNLM%3D&st=2021-02-19T00%3A11%3A20Z&se=2021-02-19T08%3A21%3A20Z&sp=r',\n",
       "  'logs/azureml/1_88_azureml.log': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/logs/azureml/1_88_azureml.log?sv=2019-02-02&sr=b&sig=yE8FldeDHCIAzqfHbBT741etMvPlDU77HC84J6XE7sU%3D&st=2021-02-19T00%3A11%3A20Z&se=2021-02-19T08%3A21%3A20Z&sp=r',\n",
       "  'logs/azureml/dataprep/backgroundProcess.log': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=zMr%2B4JgjLDafSaAMaujuy07YCffHzfs%2B2jpVfVly1%2Bs%3D&st=2021-02-19T00%3A11%3A20Z&se=2021-02-19T08%3A21%3A20Z&sp=r',\n",
       "  'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=SjtxYGoKADqzFYgGxeIXCkz51wuVe6aHtn9%2BcfYjWxY%3D&st=2021-02-19T00%3A11%3A20Z&se=2021-02-19T08%3A21%3A20Z&sp=r',\n",
       "  'logs/azureml/job_prep_azureml.log': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=LQi16N24AvCUlT9beW5DMa4FtVbrbqwLIni10rjfXF0%3D&st=2021-02-19T00%3A11%3A20Z&se=2021-02-19T08%3A21%3A20Z&sp=r',\n",
       "  'logs/azureml/job_release_azureml.log': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=l%2BEqLHhjHVyAFKFc5D7DjW1I3JBO2kMc01gZfI%2FICiw%3D&st=2021-02-19T00%3A11%3A20Z&se=2021-02-19T08%3A21%3A20Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d/all.log': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/logs/azureml/sidecar/tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d/all.log?sv=2019-02-02&sr=b&sig=%2BaMxZqU3p1MogAjfvrvMGAl5%2FOvkhJptVBtzgCdSFLY%3D&st=2021-02-19T00%3A11%3A20Z&se=2021-02-19T08%3A21%3A20Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d/task.enter_contexts.log': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/logs/azureml/sidecar/tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d/task.enter_contexts.log?sv=2019-02-02&sr=b&sig=vdLd%2FrHPWdY30XJA1Sk3Icl5OyDWDYnZEKQYFNzPTRE%3D&st=2021-02-19T00%3A11%3A20Z&se=2021-02-19T08%3A21%3A20Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d/task.exit_contexts.log': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/logs/azureml/sidecar/tvmps_3961494842bf7a51f8ef586a1b7dd63c312f2ca5ade787f9dcfe9df0f7d79a60_d/task.exit_contexts.log?sv=2019-02-02&sr=b&sig=KICG3naQE899iH4wJ44f0%2B5YhPP2WVKjgKQ4s2eQhvU%3D&st=2021-02-19T00%3A11%3A20Z&se=2021-02-19T08%3A21%3A20Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_b268f66e9e58536935eb2696e0558dc3cdad9f657f752c291ae0ca85f5955f66_d/all.log': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/logs/azureml/sidecar/tvmps_b268f66e9e58536935eb2696e0558dc3cdad9f657f752c291ae0ca85f5955f66_d/all.log?sv=2019-02-02&sr=b&sig=b8jn1L9UpYUPajFLNlmHUWJGfeYzEmPTtuInndNj2Mo%3D&st=2021-02-19T00%3A11%3A20Z&se=2021-02-19T08%3A21%3A20Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_b268f66e9e58536935eb2696e0558dc3cdad9f657f752c291ae0ca85f5955f66_d/task.enter_contexts.log': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/logs/azureml/sidecar/tvmps_b268f66e9e58536935eb2696e0558dc3cdad9f657f752c291ae0ca85f5955f66_d/task.enter_contexts.log?sv=2019-02-02&sr=b&sig=SXjq%2FF6dADa45OOjeLl%2B69uz8OUOkypWaTDeGuIrRis%3D&st=2021-02-19T00%3A11%3A20Z&se=2021-02-19T08%3A21%3A20Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_b268f66e9e58536935eb2696e0558dc3cdad9f657f752c291ae0ca85f5955f66_d/task.exit_contexts.log': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-cifar-distr_1613693036_9d4025fd/logs/azureml/sidecar/tvmps_b268f66e9e58536935eb2696e0558dc3cdad9f657f752c291ae0ca85f5955f66_d/task.exit_contexts.log?sv=2019-02-02&sr=b&sig=LphYlbzbNJeCggYBQvL%2BUsBmzbkow3bDVBJ1xmK84Mg%3D&st=2021-02-19T00%3A11%3A20Z&se=2021-02-19T08%3A21%3A20Z&sp=r'},\n",
       " 'submittedBy': 'Songshan Li'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = experiment.submit(src)\n",
    "run.wait_for_completion(show_output=True) # this provides a verbose log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "register_model_name = 'cifar10torch'\n",
    "model = run.register_model(model_name=register_model_name, model_path='outputs/cifar10torch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model named \"cifar10torch\" should be registered in your AML workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Registered Model\n",
    "\n",
    "To test the trained model, you can create (or use existing) a AKS cluster for serving the model using AML deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Workspace, Model, ComputeTarget\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision the AKS Cluster\n",
    "\n",
    "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it. It may take 5 mins to create a new AKS cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "using compute target:  aks-service-2\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'aks-service-2'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    is_new_compute  = False\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                    name = aks_name, \n",
    "                                    provisioning_configuration = prov_config)\n",
    "    is_new_compute  = True\n",
    "    \n",
    "print(\"using compute target: \", aks_target.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running.......\n",
      "Succeeded\n",
      "AKS service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "env = Environment.from_conda_specification(name='pytorch-cifar', file_path='pytorch-script/conda_dependencies.yml')\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score_pytorch.py', environment=env)\n",
    "deploy_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "#register_model_name = \"cifar10torch80\"\n",
    "model = ws.models[register_model_name]\n",
    "service_name = 'cifartorchservice4'\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=deploy_config,\n",
    "                       deployment_target=aks_target,\n",
    "                       overwrite=True)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Inputs:\n",
    "\n",
    "For testing purpose, first five images from test batch are extracted. Let's take a look these pictures: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_files = [\"test_img_0_cat.jpg\", \"test_img_1_ship.jpg\",\"test_img_2_ship.jpg\",\"test_img_3_plane.jpg\",\"test_img_4_frog.jpg\"]\n",
    "\n",
    "for img_file in img_files:\n",
    "    image = Image.open(\"test_imgs/{}\".format(img_file))\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some data process, these five images are converted to json as input for the trained model. The outputs are logits for each class per image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5153380632400513, -1.8001810312271118, 0.9319053292274475, 1.1880533695220947, 0.8720941543579102, 0.5881021022796631, 1.0242440700531006, 0.7531565427780151, -1.9846534729003906, -1.846501350402832], [5.626550674438477, 5.266762733459473, -0.9428859949111938, -3.697542667388916, -2.906374454498291, -6.4479451179504395, -2.991352081298828, -2.7618331909179688, 5.915030479431152, 2.597166061401367], [4.207612991333008, 1.9836726188659668, 0.22895176708698273, -2.0511369705200195, -1.7314388751983643, -4.231654167175293, -2.3347039222717285, -1.3680027723312378, 3.997978687286377, 1.2459831237792969], [3.6119861602783203, 2.680774450302124, 0.2790420949459076, -1.8072761297225952, -2.1849260330200195, -3.877857208251953, -2.5081701278686523, -1.5793991088867188, 3.8795742988586426, 1.4604357481002808], [-2.149258613586426, -3.239884614944458, 2.0069427490234375, 1.7941792011260986, 2.4163215160369873, 0.6278948783874512, 3.9332661628723145, 0.04551582783460617, -3.313084125518799, -2.978672981262207]]\n"
     ]
    }
   ],
   "source": [
    "with open(\"cifar_test_input_pytorch.json\", \"r\") as fp:\n",
    "    inputs_json = json.load(fp)\n",
    "    \n",
    "inputs = json.dumps(inputs_json)\n",
    "resp = service.run(inputs)\n",
    "predicts = resp[\"predicts\"]\n",
    "print(predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can easily get the predictions of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'ship', 'plane', 'ship', 'frog']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "np_predicts = np.array(predicts)\n",
    "pred_indexes = np.argmax(np_predicts, 1)\n",
    "\n",
    "predict_labels = [classes[i] for i in pred_indexes]\n",
    "print(predict_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the newly created cluster\n",
    "\n",
    "Note: This is important if you wish to avoid the cost of this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_new_compute:\n",
    "    aks_target.delete()\n",
    "    service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Learn how to [download model then upload to Azure Storage blobs](../AML-model-download-upload.ipynb)\n",
    "2. Learn how to [inference using KFServing with model in Azure Storage Blobs](https://aka.ms/kfas)\n",
    "3. Learn Pipeline Steps with [Object Segmentation](../object-segmentation-on-azure-stack/)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "ninhu"
   }
  ],
  "category": "training",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "MNIST"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "PyTorch"
  ],
  "friendly_name": "Distributed training with PyTorch",
  "index_order": 1,
  "kernelspec": {
   "display_name": "pytouchEnv",
   "language": "python",
   "name": "pytouchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "tags": [
   "None"
  ],
  "task": "Train a model using distributed training via Nccl/Gloo"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
