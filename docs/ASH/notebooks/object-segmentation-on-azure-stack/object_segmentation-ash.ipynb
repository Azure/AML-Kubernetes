{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Segmenation on Azure Stack Hub Clusters\n",
    "\n",
    "For this tutorial, we will fine tune a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model in the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/). It contains 170 images with 345 instances of pedestrians, and we will use it  to train an instance segmentation model on a custom dataset.\n",
    "\n",
    "\n",
    "You will use [Azure Machine Learning Pipelines](https://aka.ms/aml-pipelines) to define two pipeline steps: a data process step which split data into training and testing, and training step which trains and evaluates the model.  The trained model then registered to your AML workspace.\n",
    "\n",
    "\n",
    "After the model is registered, you then deploy the model for serving or testing. You will deploy the model to different compute platform: 1) Azure Kubernetes Cluster (AKS), 2) your local computer\n",
    "\n",
    "This is a notebook about using ASH storage and ASH cluster (ARC compute) for training and serving, please make sure the following prerequisites are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "*     A Kubernetes cluster deployed on Azure Stack Hub, connected to Azure through ARC.\n",
    "     \n",
    "   To create a Kubernetes cluster on Azure Stack Hub, please see [here](https://docs.microsoft.com/en-us/azure-stack/user/azure-stack-kubernetes-aks-engine-overview?view=azs-2008).\n",
    "\n",
    "\n",
    "*     Connect  Azure Stack Hubâ€™s Kubernetes cluster to  Azure via [ Azure ARC](https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/connect-cluster).(For installation, please read note below first)\n",
    "    \n",
    " Important Note: This notebook  requires az extension k8s-extension >= 0.1 and connectedk8s >= 0.3.2 installed on the cluster's master node. Current version for connectedk8s in public preview release via [ Azure ARC](https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/connect-cluster) is 0.2.8, so you need to install [private preview](https://github.com/Azure/azure-arc-kubernetes-preview/blob/master/docs/k8s-extensions.md). For your convenience, we have include the wheel files, and you just need to run:\n",
    " \n",
    " <pre>\n",
    " az extension add --source connectedk8s-0.3.5-py2.py3-none-any.whl --yes\n",
    " az extension add --source k8s_extension-0.1PP.8-py2.py3-none-any.whl --yes\n",
    " </pre>\n",
    "  \n",
    "\n",
    "*     A storage account deployed on Azure Stack Hub.\n",
    "\n",
    "\n",
    "*     Setup Azure Machine Learning workspace on Azure.\n",
    "\n",
    "   please make sure the following \n",
    "   [Prerequisites](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python#prerequisites) are met (we recommend using the Python SDK when communicating with Azure Machine Learning so make sure the SDK is properly installed). We strongly recommend learning more about the [innerworkings and concepts in Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture) before continuing with the rest of this article (optional)\n",
    "\n",
    "\n",
    "*      Last but not least, you need to be able to run a Notebook. \n",
    "\n",
    "   If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace,Environment, Experiment, Datastore\n",
    "\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.runconfig import RunConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "\n",
    "In preparation stage, you will create a AML workspace first, then\n",
    "\n",
    "*  Create a compute target for AML workspace by attaching the cluster deployed on Azure Stack Hub (ASH)\n",
    "\n",
    "*  Create a datastore for AML workspace backed by storage account deployed on ASH.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create Compute Target by attaching cluster deployed on ASH\n",
    "\n",
    "The attaching code here depends  python package azureml-contrib-k8s which current is in private preview. Install private preview branch of AzureML SDK by running following command (private preview):\n",
    "\n",
    "<pre>\n",
    "pip install --disable-pip-version-check --extra-index-url https://azuremlsdktestpypi.azureedge.net/azureml-contrib-k8s-preview/D58E86006C65 azureml-contrib-k8s\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.core.compute.arckubernetescompute import ArcKubernetesCompute\n",
    "\n",
    "resource_id = \"/subscriptions/6b736da6-3246-44dd-a0b8-b5e95484633d/resourceGroups/AML-stack-val/providers/Microsoft.Kubernetes/connectedClusters/kub-orlando-Test\"\n",
    "\n",
    "attach_name = \"peymanarc\"\n",
    "\n",
    "attach_config = ArcKubernetesCompute.attach_configuration(resource_id=resource_id)\n",
    "\n",
    "attach_result = ArcKubernetesCompute.attach(ws, attach_name, attach_config)\n",
    "\n",
    "attach_result.wait_for_completion(show_output=True)\n",
    "\n",
    "print(attach_result)\n",
    "\n",
    "compute_target = ws.compute_targets[attach_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datastore for AML workspace backed by storage account deployed on ASH\n",
    "\n",
    "Here is the [instruction](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset\n",
    "\n",
    "After downloading and extracting the zip file from [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/) to your local machine, you will have the following folder structure:\n",
    "\n",
    "<pre>\n",
    "PennFudanPed/\n",
    "  PedMasks/\n",
    "    FudanPed00001_mask.png\n",
    "    FudanPed00002_mask.png\n",
    "    FudanPed00003_mask.png\n",
    "    FudanPed00004_mask.png\n",
    "    ...\n",
    "  PNGImages/\n",
    "    FudanPed00001.png\n",
    "    FudanPed00002.png\n",
    "    FudanPed00003.png\n",
    "    FudanPed00004.png\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "\n",
    "dataset_name = \"pennfudan_2\"\n",
    "if dataset_name not  in ws.datasets:\n",
    "    \n",
    "    datastore_name = \"ashstore\"\n",
    "    datastore =  Datastore.get(ws, datastore_name)\n",
    "    \n",
    "    src_dir, target_path = 'PennFudanPed', 'PennFudanPed'\n",
    "    datastore.upload(src_dir, target_path)\n",
    "\n",
    "    # register data uploaded as AML dataset\n",
    "    datastore_paths = [(datastore, target_path)]\n",
    "    pd_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "    pd_ds.register(ws, dataset_name, \"for Pedestrian Detection and Segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Training-Test split data process Step\n",
    "\n",
    "For this pipeline run, you will use two pipeline steps.  The first step is to split dataset into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create run_config first\n",
    "datastore_name = \"ashstore\"\n",
    "datastore =  Datastore.get(ws, datastore_name)\n",
    "\n",
    "env = Environment.from_dockerfile(\n",
    "        name='pytorch-obj-seg',\n",
    "        dockerfile='./aml_src/Dockerfile.gpu',\n",
    "        conda_specification='./aml_src/conda-env.yaml')\n",
    "\n",
    "aml_run_config = RunConfiguration()\n",
    "aml_run_config.target = compute_target.name\n",
    "aml_run_config.environment = env\n",
    "\n",
    "source_directory = './aml_src'\n",
    "\n",
    "\n",
    "# add a data process step\n",
    "\n",
    "dataset = ws.datasets[dataset_name]\n",
    "\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "dest = (datastore, None)\n",
    "\n",
    "train_split_data = OutputFileDatasetConfig(name=\"train_split_data\", destination=dest).as_upload(overwrite=False)\n",
    "test_split_data = OutputFileDatasetConfig(name=\"test_split_data\", destination=dest).as_upload(overwrite=False)\n",
    "\n",
    "split_step = PythonScriptStep(\n",
    "    name=\"Train Test Split\",\n",
    "    script_name=\"obj_segment_step_data_process.py\",\n",
    "    arguments=[\"--data-path\", dataset.as_named_input('pennfudan_data').as_mount(),\n",
    "               \"--train-split\", train_split_data, \"--test-split\", test_split_data,\n",
    "               \"--test-size\", 50],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=source_directory,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(\n",
    "        name=\"training_step\",\n",
    "        script_name=\"obj_segment_step_training.py\",\n",
    "        arguments=[\n",
    "            \"--train-split\", train_split_data.as_input(), \"--test-split\", test_split_data.as_input(),\n",
    "            '--epochs', 1,  # 80\n",
    "        ],\n",
    "\n",
    "        compute_target=compute_target,\n",
    "        runconfig=aml_run_config,\n",
    "        source_directory=source_directory,\n",
    "        allow_reuse=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment and Submit Pipeline Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'obj_seg_step'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "pipeline_steps = [train_step]\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n",
    "pipeline_run.wait_for_completion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model\n",
    "\n",
    "Note: Here we saved and register two models. The model saved at \"'outputs/obj_segmentation.pkl'\" is registered as   'obj_seg_model_aml'.  It contains both model parameters and network which are used by AML deployment and serving.  Model 'obj_seg_model_kf_torch' contains only parameter values which maybe used for KFServing as shown in [this notebook](object_segmentation_kfserving.ipynb) If you are not interesting in KFServing, you can safely ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_run = pipeline_run.find_step_run(train_step.name)[0]\n",
    "\n",
    "model_name = 'obj_seg_model_aml' # model for AML serving\n",
    "train_step_run.register_model(model_name=model_name, model_path='outputs/obj_segmentation.pkl')\n",
    "\n",
    "model_name = 'obj_seg_model_kf_torch' # model for KFServing using pytorchserver\n",
    "train_step_run.register_model(model_name=model_name, model_path='outputs/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Registered Model\n",
    "\n",
    "To test the trained model, you can create (or use existing) a AKS cluster for serving the model using AML deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create (or use existing) a AKS cluster for serving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Workspace, Model, ComputeTarget\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'aks-service-2'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    is_new_compute  = False\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws,\n",
    "                                    name = aks_name,\n",
    "                                    provisioning_configuration = prov_config)\n",
    "    is_new_compute  = True\n",
    "\n",
    "if aks_target.get_status() != \"Succeeded\":\n",
    "    aks_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.from_dockerfile(\n",
    "        name='pytorch-obj-seg',\n",
    "        dockerfile='./aml_src/Dockerfile.gpu',\n",
    "        conda_specification='./aml_src/conda-env.yaml')\n",
    "\n",
    "env.inferencing_stack_version='latest'\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
    "deploy_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "deployed_model = \"obj_seg_model_aml\" # model_name\n",
    "model = ws.models[deployed_model]\n",
    "\n",
    "service_name = 'objservice'\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=deploy_config,\n",
    "                       deployment_target=aks_target,\n",
    "                       overwrite=True)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model Using the Deployed Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nums = [\"00001\"]\n",
    "image_paths = [\"PennFudanPed\\\\PNGImages\\\\FudanPed{}.png\".format(item) for item in img_nums]\n",
    "image_np_list = []\n",
    "for image_path in image_paths:\n",
    "    img = Image.open(image_path)\n",
    "    img.show(\"input_image\")\n",
    "    img_rgb = img.convert(\"RGB\")\n",
    "    img_tensor = F.to_tensor(img_rgb)\n",
    "    img_np = img_tensor.numpy()\n",
    "    image_np_list.append(img_np.tolist())\n",
    "\n",
    "inputs = json.dumps({\"instances\": image_np_list})\n",
    "resp = service.run(inputs)\n",
    "predicts = resp[\"predictions\"]\n",
    "p_str = json.dumps(predicts)\n",
    "\n",
    "p_obj = json.loads(p_str)\n",
    "\n",
    "for instance_pred in p_obj:\n",
    "    image_data = instance_pred[\"masks\"]\n",
    "    img_np = np.array(image_data)\n",
    "    output = Image.fromarray(img_np)\n",
    "    output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the newly created cluster\n",
    "\n",
    "Note: This is important if you wish to avoid the cost of this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_new_compute:\n",
    "    aks_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "pythonproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
