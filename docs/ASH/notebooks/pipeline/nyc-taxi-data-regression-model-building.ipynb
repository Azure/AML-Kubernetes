{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/nyc-taxi-data-regression-model-building/nyc-taxi-data-regression-model-building.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Pipelines With Azure Stack K8s Clusters\n",
    "\n",
    "In this notebook, you learn how to prepare data for regression modeling by using open source library [pandas](https://pandas.pydata.org/). You run various transformations to filter and combine two different NYC taxi datasets. Once you prepare the NYC taxi data for regression modeling, then you will use [Azure Machine Learning Pipelines](https://aka.ms/aml-pipelines) to define your machine learning goals and constraints as well a custom training.  The trained model then registered to AML workspace.\n",
    "\n",
    "After you complete building the model, you can predict the cost of a taxi trip by training a model on data features. These features include the pickup day and time, the number of passengers, and the pickup location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "*     A Kubernetes cluster deployed on Azure Stack Hub, connected to Azure through ARC.\n",
    "     \n",
    "   To deploy a Kubernetes cluster with AKS engine on Azure Stack Hub, please see [here](https://docs.microsoft.com/en-us/azure-stack/user/azure-stack-kubernetes-aks-engine-overview?view=azs-2008).\n",
    "\n",
    "\n",
    "*     Connect  Azure Stack Hubâ€™s Kubernetes cluster to  Azure via [ Azure ARC](https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/connect-cluster).(For installation, please read note below first)\n",
    "    \n",
    " Important Note: This notebook  requires az extension k8s-extension >= 0.1 and connectedk8s >= 0.3.2 installed on the cluster's master node. Current version for connectedk8s in public preview release via [ Azure ARC](https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/connect-cluster) is 0.2.8, so you need to install [private preview](https://github.com/Azure/azure-arc-kubernetes-preview/blob/master/docs/k8s-extensions.md). For your convenience, we have include the wheel files, and you just need to run:\n",
    " \n",
    " <pre>\n",
    " az extension add --source connectedk8s-0.3.5-py2.py3-none-any.whl --yes\n",
    " az extension add --source k8s_extension-0.1PP.8-py2.py3-none-any.whl --yes\n",
    " </pre>\n",
    "  \n",
    "\n",
    "*     Setup Azure Machine Learning workspace on Azure.\n",
    "\n",
    "   please make sure the following \n",
    "   [Prerequisites](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python#prerequisites) are met (we recommend using the Python SDK when communicating with Azure Machine Learning so make sure the SDK is properly installed). We strongly recommend learning more about the [innerworkings and concepts in Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture) before continuing with the rest of this article (optional)\n",
    "\n",
    "\n",
    "*      Last but not least, you need to be able to run a Notebook. \n",
    "\n",
    "   If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for regression modeling\n",
    "First, we will prepare data for regression modeling. We will leverage the convenience of Azure Open Datasets along with the power of Azure Machine Learning service to create a regression model to predict NYC taxi fare prices. Perform `pip install azureml-opendatasets` to get the open dataset package.  The Open Datasets package contains a class representing each data source (NycTlcGreen and NycTlcYellow) to easily filter date parameters before downloading.\n",
    "\n",
    "\n",
    "### Load data\n",
    "Begin by creating a dataframe to hold the taxi data. When working in a non-Spark environment, Open Datasets only allows downloading one month of data at a time with certain classes to avoid MemoryError with large datasets. To download a year of taxi data, iteratively fetch one month at a time, and before appending it to green_df_raw, randomly sample 500 records from each month to avoid bloating the dataframe. Then preview the data. To keep this process short, we are sampling data of only 1 month.\n",
    "\n",
    "Note: Open Datasets has mirroring classes for working in Spark environments where data size and memory aren't a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (azureml-telemetry 1.19.0 (c:\\users\\v-songshanli\\anaconda3\\envs\\pythonproject\\lib\\site-packages), Requirement.parse('azureml-telemetry~=1.18.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (azureml-core 1.19.0 (c:\\users\\v-songshanli\\anaconda3\\envs\\pythonproject\\lib\\site-packages), Requirement.parse('azureml-core~=1.18.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.19.0 (c:\\users\\v-songshanli\\anaconda3\\envs\\pythonproject\\lib\\site-packages), Requirement.parse('azureml-core~=1.18.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.19.0 (c:\\users\\v-songshanli\\anaconda3\\envs\\pythonproject\\lib\\site-packages), Requirement.parse('azureml-core~=1.18.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.19.0 (c:\\users\\v-songshanli\\anaconda3\\envs\\pythonproject\\lib\\site-packages), Requirement.parse('azureml-core~=1.18.0')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.19.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp404nwpy4\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\green\\puYear=2016\\puMonth=1\\part-00119-tid-4753095944193949832-fee7e113-666d-4114-9fcb-bcd3046479f3-2689-1.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "from azureml.opendatasets import NycTlcGreen, NycTlcYellow\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "green_df_raw = pd.DataFrame([])\n",
    "start = datetime.strptime(\"1/1/2016\",\"%m/%d/%Y\")\n",
    "end = datetime.strptime(\"1/31/2016\",\"%m/%d/%Y\")\n",
    "\n",
    "number_of_months = 1\n",
    "sample_size = 5000\n",
    "\n",
    "for sample_month in range(number_of_months):\n",
    "    temp_df_green = NycTlcGreen(start + relativedelta(months=sample_month), end + relativedelta(months=sample_month)) \\\n",
    "        .to_pandas_dataframe()\n",
    "    green_df_raw = green_df_raw.append(temp_df_green.sample(sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00000-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426339-90.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00001-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426336-89.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00002-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426334-91.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00003-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426340-87.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00004-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426331-88.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00005-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426324-89.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00006-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426326-88.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00007-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426332-90.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00008-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426341-90.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00009-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426325-88.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00010-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426335-89.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00011-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426338-89.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00012-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426337-89.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00013-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426327-89.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00014-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426330-90.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00015-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426342-89.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00016-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426328-88.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00017-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426323-90.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00018-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426329-90.c000.snappy.parquet\n",
      "[Info] read from C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\tmp5ny3ja2o\\https%3A\\%2Fazureopendatastorage.azurefd.net\\nyctlc\\yellow\\puYear=2016\\puMonth=1\\part-00019-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426333-88.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "yellow_df_raw = pd.DataFrame([])\n",
    "start = datetime.strptime(\"1/1/2016\",\"%m/%d/%Y\")\n",
    "end = datetime.strptime(\"1/31/2016\",\"%m/%d/%Y\")\n",
    "\n",
    "sample_size = 500\n",
    "\n",
    "for sample_month in range(number_of_months):\n",
    "    temp_df_yellow = NycTlcYellow(start + relativedelta(months=sample_month), end + relativedelta(months=sample_month)) \\\n",
    "        .to_pandas_dataframe()\n",
    "    yellow_df_raw = yellow_df_raw.append(temp_df_yellow.sample(sample_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendorID</th>\n",
       "      <th>lpepPickupDatetime</th>\n",
       "      <th>lpepDropoffDatetime</th>\n",
       "      <th>passengerCount</th>\n",
       "      <th>tripDistance</th>\n",
       "      <th>puLocationId</th>\n",
       "      <th>doLocationId</th>\n",
       "      <th>pickupLongitude</th>\n",
       "      <th>pickupLatitude</th>\n",
       "      <th>dropoffLongitude</th>\n",
       "      <th>...</th>\n",
       "      <th>extra</th>\n",
       "      <th>mtaTax</th>\n",
       "      <th>improvementSurcharge</th>\n",
       "      <th>tipAmount</th>\n",
       "      <th>tollsAmount</th>\n",
       "      <th>ehailFee</th>\n",
       "      <th>totalAmount</th>\n",
       "      <th>tripType</th>\n",
       "      <th>puYear</th>\n",
       "      <th>puMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1052053</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-25 11:16:30</td>\n",
       "      <td>2016-01-25 11:34:24</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-73.942245</td>\n",
       "      <td>40.700855</td>\n",
       "      <td>-73.964294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072509</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-25 19:41:35</td>\n",
       "      <td>2016-01-25 19:45:02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.54</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-73.964577</td>\n",
       "      <td>40.807301</td>\n",
       "      <td>-73.969955</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300352</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-27 11:37:05</td>\n",
       "      <td>2016-01-27 11:42:18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-73.959389</td>\n",
       "      <td>40.711594</td>\n",
       "      <td>-73.957497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492404</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-09 16:10:59</td>\n",
       "      <td>2016-01-09 16:13:49</td>\n",
       "      <td>3</td>\n",
       "      <td>0.40</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-73.939232</td>\n",
       "      <td>40.810085</td>\n",
       "      <td>-73.943802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629692</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-17 18:29:34</td>\n",
       "      <td>2016-01-17 18:37:08</td>\n",
       "      <td>1</td>\n",
       "      <td>1.95</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-73.954994</td>\n",
       "      <td>40.805008</td>\n",
       "      <td>-73.945343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         vendorID  lpepPickupDatetime lpepDropoffDatetime  passengerCount  \\\n",
       "1052053         2 2016-01-25 11:16:30 2016-01-25 11:34:24               1   \n",
       "1072509         2 2016-01-25 19:41:35 2016-01-25 19:45:02               1   \n",
       "300352          2 2016-01-27 11:37:05 2016-01-27 11:42:18               1   \n",
       "492404          1 2016-01-09 16:10:59 2016-01-09 16:13:49               3   \n",
       "629692          2 2016-01-17 18:29:34 2016-01-17 18:37:08               1   \n",
       "\n",
       "         tripDistance puLocationId doLocationId  pickupLongitude  \\\n",
       "1052053          1.50         None         None       -73.942245   \n",
       "1072509          0.54         None         None       -73.964577   \n",
       "300352           0.87         None         None       -73.959389   \n",
       "492404           0.40         None         None       -73.939232   \n",
       "629692           1.95         None         None       -73.954994   \n",
       "\n",
       "         pickupLatitude  dropoffLongitude  ...  extra  mtaTax  \\\n",
       "1052053       40.700855        -73.964294  ...    0.0     0.5   \n",
       "1072509       40.807301        -73.969955  ...    1.0     0.5   \n",
       "300352        40.711594        -73.957497  ...    0.0     0.5   \n",
       "492404        40.810085        -73.943802  ...    0.0     0.0   \n",
       "629692        40.805008        -73.945343  ...    0.0     0.5   \n",
       "\n",
       "        improvementSurcharge  tipAmount  tollsAmount  ehailFee  totalAmount  \\\n",
       "1052053                  0.3       0.00          0.0       NaN        12.80   \n",
       "1072509                  0.3       0.00          0.0       NaN         6.30   \n",
       "300352                   0.3       1.26          0.0       NaN         7.56   \n",
       "492404                     0       0.00          0.0       NaN         7.00   \n",
       "629692                   0.3       1.86          0.0       NaN        11.16   \n",
       "\n",
       "        tripType  puYear  puMonth  \n",
       "1052053      1.0    2016        1  \n",
       "1072509      1.0    2016        1  \n",
       "300352       1.0    2016        1  \n",
       "492404       2.0    2016        1  \n",
       "629692       1.0    2016        1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendorID</th>\n",
       "      <th>tpepPickupDateTime</th>\n",
       "      <th>tpepDropoffDateTime</th>\n",
       "      <th>passengerCount</th>\n",
       "      <th>tripDistance</th>\n",
       "      <th>puLocationId</th>\n",
       "      <th>doLocationId</th>\n",
       "      <th>startLon</th>\n",
       "      <th>startLat</th>\n",
       "      <th>endLon</th>\n",
       "      <th>...</th>\n",
       "      <th>paymentType</th>\n",
       "      <th>fareAmount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mtaTax</th>\n",
       "      <th>improvementSurcharge</th>\n",
       "      <th>tipAmount</th>\n",
       "      <th>tollsAmount</th>\n",
       "      <th>totalAmount</th>\n",
       "      <th>puYear</th>\n",
       "      <th>puMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130780</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-07 08:11:41</td>\n",
       "      <td>2016-01-07 08:22:20</td>\n",
       "      <td>6</td>\n",
       "      <td>1.62</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-73.994659</td>\n",
       "      <td>40.740490</td>\n",
       "      <td>-73.974243</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.76</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184083</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 21:05:04</td>\n",
       "      <td>2016-01-19 21:13:43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-73.968651</td>\n",
       "      <td>40.761150</td>\n",
       "      <td>-73.983559</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.38</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129122</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-16 23:42:08</td>\n",
       "      <td>2016-01-16 23:50:21</td>\n",
       "      <td>5</td>\n",
       "      <td>1.24</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-73.965080</td>\n",
       "      <td>40.755379</td>\n",
       "      <td>-73.979362</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.56</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308992</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-27 13:43:07</td>\n",
       "      <td>2016-01-27 13:55:26</td>\n",
       "      <td>1</td>\n",
       "      <td>1.17</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-73.962257</td>\n",
       "      <td>40.779037</td>\n",
       "      <td>-73.961945</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.80</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359617</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-30 19:50:07</td>\n",
       "      <td>2016-01-30 19:55:17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-73.959457</td>\n",
       "      <td>40.771736</td>\n",
       "      <td>-73.947731</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       vendorID  tpepPickupDateTime tpepDropoffDateTime  passengerCount  \\\n",
       "130780        2 2016-01-07 08:11:41 2016-01-07 08:22:20               6   \n",
       "184083        2 2016-01-19 21:05:04 2016-01-19 21:13:43               1   \n",
       "129122        2 2016-01-16 23:42:08 2016-01-16 23:50:21               5   \n",
       "308992        2 2016-01-27 13:43:07 2016-01-27 13:55:26               1   \n",
       "359617        1 2016-01-30 19:50:07 2016-01-30 19:55:17               1   \n",
       "\n",
       "        tripDistance puLocationId doLocationId   startLon   startLat  \\\n",
       "130780          1.62         None         None -73.994659  40.740490   \n",
       "184083          0.98         None         None -73.968651  40.761150   \n",
       "129122          1.24         None         None -73.965080  40.755379   \n",
       "308992          1.17         None         None -73.962257  40.779037   \n",
       "359617          0.80         None         None -73.959457  40.771736   \n",
       "\n",
       "           endLon  ...  paymentType  fareAmount extra mtaTax  \\\n",
       "130780 -73.974243  ...            1         9.0   0.0    0.5   \n",
       "184083 -73.983559  ...            1         7.0   0.5    0.5   \n",
       "129122 -73.979362  ...            1         7.5   0.5    0.5   \n",
       "308992 -73.961945  ...            2         9.0   0.0    0.5   \n",
       "359617 -73.947731  ...            2         5.5   0.0    0.5   \n",
       "\n",
       "        improvementSurcharge  tipAmount  tollsAmount totalAmount  puYear  \\\n",
       "130780                   0.3       1.96          0.0       11.76    2016   \n",
       "184083                   0.3       2.08          0.0       10.38    2016   \n",
       "129122                   0.3       1.76          0.0       10.56    2016   \n",
       "308992                   0.3       0.00          0.0        9.80    2016   \n",
       "359617                   0.3       0.00          0.0        6.30    2016   \n",
       "\n",
       "        puMonth  \n",
       "130780        1  \n",
       "184083        1  \n",
       "129122        1  \n",
       "308992        1  \n",
       "359617        1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(green_df_raw.head(5))\n",
    "display(yellow_df_raw.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data locally and then upload to Azure Blob\n",
    "\n",
    "This is a one-time process to save the data in the default datastore. \n",
    "\n",
    "To set up datastore using an azure stack hub storage account, please refer to [Train_azure_arc](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md#create-and-configure-azure-stack-hubs-storage-account). \n",
    "\n",
    "To register the dataset manually, please refer to this [video](https://msit.microsoftstream.com/video/51f7a3ff-0400-b9eb-2703-f1eb38bc6232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to local folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataDir = \"data\"\n",
    "\n",
    "if not os.path.exists(dataDir):\n",
    "    os.mkdir(dataDir)\n",
    "\n",
    "greenDir = dataDir + \"/green\"\n",
    "yelloDir = dataDir + \"/yellow\"\n",
    "\n",
    "if not os.path.exists(greenDir):\n",
    "    os.mkdir(greenDir)\n",
    "    \n",
    "if not os.path.exists(yelloDir):\n",
    "    os.mkdir(yelloDir)\n",
    "    \n",
    "greenTaxiData = greenDir + \"/unprepared.parquet\"\n",
    "yellowTaxiData = yelloDir + \"/unprepared.parquet\"\n",
    "\n",
    "green_df_raw.to_csv(greenTaxiData, index=False)\n",
    "yellow_df_raw.to_csv(yellowTaxiData, index=False)\n",
    "\n",
    "print(\"Data written to local folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace: sl-ash2-mal\n",
      "Region: eastus\n",
      "Uploading an estimated of 1 files\n",
      "Uploading data/green/unprepared.parquet\n",
      "Uploaded data/green/unprepared.parquet, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Uploading an estimated of 1 files\n",
      "Uploading data/yellow/unprepared.parquet\n",
      "Uploaded data/yellow/unprepared.parquet, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Upload calls completed.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(\"Workspace: \" + ws.name, \"Region: \" + ws.location, sep = '\\n')\n",
    "\n",
    "# Default datastore\n",
    "\n",
    "#datastore_name = \"ashstore\"\n",
    "#default_store =  Datastore.get(ws, datastore_name)\n",
    "\n",
    "default_store = ws.get_default_datastore() \n",
    "\n",
    "default_store.upload_files([greenTaxiData], \n",
    "                           target_path = 'green', \n",
    "                           overwrite = True, \n",
    "                           show_progress = True)\n",
    "\n",
    "default_store.upload_files([yellowTaxiData], \n",
    "                           target_path = 'yellow', \n",
    "                           overwrite = True, \n",
    "                           show_progress = True)\n",
    "\n",
    "print(\"Upload calls completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and register datasets\n",
    "\n",
    "By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. You can learn more about the what subsetting capabilities are supported by referring to [documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.tabular_dataset.tabulardataset?view=azure-ml-py#remarks). The data remains in its existing location, so no extra storage cost is incurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "green_taxi_data = Dataset.Tabular.from_delimited_files(default_store.path('green/unprepared.parquet'))\n",
    "yellow_taxi_data = Dataset.Tabular.from_delimited_files(default_store.path('yellow/unprepared.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the taxi datasets with the workspace so that you can reuse them in other experiments or share with your colleagues who have access to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_taxi_data = green_taxi_data.register(ws, 'green_taxi_data')\n",
    "yellow_taxi_data = yellow_taxi_data.register(ws, 'yellow_taxi_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Compute By Attaching a ARC Kubernetes Cluster\n",
    "\n",
    "The attaching code here depends  python package azureml-contrib-k8s which current is in private preview. Install private preview branch of AzureML SDK by running following command (private preview):\n",
    "\n",
    "<pre>\n",
    "pip install --disable-pip-version-check --extra-index-url https://azuremlsdktestpypi.azureedge.net/azureml-contrib-k8s-preview/D58E86006C65 azureml-contrib-k8s\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.core.compute.arckubernetescompute import ArcKubernetesCompute\n",
    "\n",
    "attach_name = \"sl-d2-o-arc\"\n",
    "aml_compute = ArcKubernetesCompute(ws, attach_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define RunConfig for the compute\n",
    "We will also use `pandas`, `scikit-learn`,  `pyarrow` for the pipeline steps. Defining the `runconfig` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a new runconfig object\n",
    "aml_run_config = RunConfiguration()\n",
    "\n",
    "# Use the aml_compute you created above. \n",
    "aml_run_config.target = aml_compute\n",
    "\n",
    "# Enable Docker\n",
    "aml_run_config.environment.docker.enabled = True\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=['pandas','scikit-learn'], \n",
    "    pip_packages=['azureml-sdk', 'pyarrow'])\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "Now we will prepare for regression modeling by using `pandas`. We run various transformations to filter and combine two different NYC taxi datasets.\n",
    "\n",
    "We achieve this by creating a separate step for each transformation as this allows us to reuse the steps and saves us from running all over again in case of any change. We will keep data preparation scripts in one subfolder and training scripts in another.\n",
    "\n",
    "> The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Useful Columns\n",
    "Here we are defining a set of \"useful\" columns for both Green and Yellow taxi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['vendorID', 'lpepPickupDatetime', 'lpepDropoffDatetime',\n",
       "       'passengerCount', 'tripDistance', 'puLocationId', 'doLocationId',\n",
       "       'pickupLongitude', 'pickupLatitude', 'dropoffLongitude',\n",
       "       'dropoffLatitude', 'rateCodeID', 'storeAndFwdFlag', 'paymentType',\n",
       "       'fareAmount', 'extra', 'mtaTax', 'improvementSurcharge', 'tipAmount',\n",
       "       'tollsAmount', 'ehailFee', 'totalAmount', 'tripType', 'puYear',\n",
       "       'puMonth'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['vendorID', 'tpepPickupDateTime', 'tpepDropoffDateTime',\n",
       "       'passengerCount', 'tripDistance', 'puLocationId', 'doLocationId',\n",
       "       'startLon', 'startLat', 'endLon', 'endLat', 'rateCodeId',\n",
       "       'storeAndFwdFlag', 'paymentType', 'fareAmount', 'extra', 'mtaTax',\n",
       "       'improvementSurcharge', 'tipAmount', 'tollsAmount', 'totalAmount',\n",
       "       'puYear', 'puMonth'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useful columns defined.\n"
     ]
    }
   ],
   "source": [
    "display(green_df_raw.columns)\n",
    "display(yellow_df_raw.columns)\n",
    "\n",
    "# useful columns needed for the Azure Machine Learning NYC Taxi tutorial\n",
    "useful_columns = str([\"cost\", \"distance\", \"dropoff_datetime\", \"dropoff_latitude\", \n",
    "                      \"dropoff_longitude\", \"passengers\", \"pickup_datetime\", \n",
    "                      \"pickup_latitude\", \"pickup_longitude\", \"store_forward\", \"vendor\"]).replace(\",\", \";\")\n",
    "\n",
    "print(\"Useful columns defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanse Green taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanse script is in C:\\Users\\v-songshanli\\projects\\AML-Kubernetes\\docs\\ASH\\notebooks\\pipeline\\scripts\\prepdata.\n",
      "cleansingStepGreen created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# python scripts folder\n",
    "prepare_data_folder = './scripts/prepdata'\n",
    "\n",
    "# rename columns as per Azure Machine Learning NYC Taxi tutorial\n",
    "\n",
    "green_columns = {\n",
    "        \"vendorID\": \"vendor\",\n",
    "        \"lpepPickupDatetime\": \"pickup_datetime\",\n",
    "        \"lpepDropoffDatetime\": \"dropoff_datetime\",\n",
    "        \"storeAndFwdFlag\": \"store_forward\",\n",
    "        \"pickupLongitude\": \"pickup_longitude\",\n",
    "        \"pickupLatitude\": \"pickup_latitude\",\n",
    "        \"dropoffLongitude\": \"dropoff_longitude\",\n",
    "        \"dropoffLatitude\": \"dropoff_latitude\",\n",
    "        \"passengerCount\": \"passengers\",\n",
    "        \"fareAmount\": \"cost\",\n",
    "        \"tripDistance\": \"distance\"\n",
    "    }\n",
    "\n",
    "green_columns_key = str(list(green_columns.keys())).replace(\",\", \";\")\n",
    "green_columns_value = str(list(green_columns.values())).replace(\",\", \";\")\n",
    "    \n",
    "# Define output after cleansing step\n",
    "cleansed_green_data = PipelineData(\"cleansed_green_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Cleanse script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# cleansing step creation\n",
    "# See the cleanse.py for details about input and output\n",
    "cleansingStepGreen = PythonScriptStep(\n",
    "    name=\"Cleanse Green Taxi Data\",\n",
    "    script_name=\"cleanse.py\", \n",
    "    arguments=[\"--useful_columns\", useful_columns,\n",
    "               \"--columns_key\", green_columns_key,\n",
    "                \"--columns_value\", green_columns_value,\n",
    "               \"--output_cleanse\", cleansed_green_data],\n",
    "    inputs=[green_taxi_data.as_named_input('raw_data')],\n",
    "    outputs=[cleansed_green_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"cleansingStepGreen created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanse Yellow taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanse script is in C:\\Users\\v-songshanli\\projects\\AML-Kubernetes\\docs\\ASH\\notebooks\\pipeline\\scripts\\prepdata.\n",
      "cleansingStepYellow created.\n"
     ]
    }
   ],
   "source": [
    "yellow_columns = {\n",
    "    \"vendorID\": \"vendor\",\n",
    "    \"tpepPickupDateTime\": \"pickup_datetime\",\n",
    "    \"tpepDropoffDateTime\": \"dropoff_datetime\",\n",
    "    \"storeAndFwdFlag\": \"store_forward\",\n",
    "    \"startLon\": \"pickup_longitude\",\n",
    "    \"startLat\": \"pickup_latitude\",\n",
    "    \"endLon\": \"dropoff_longitude\",\n",
    "    \"endLat\": \"dropoff_latitude\",\n",
    "    \"passengerCount\": \"passengers\",\n",
    "    \"fareAmount\": \"cost\",\n",
    "    \"tripDistance\": \"distance\"\n",
    "}\n",
    "\n",
    "yellow_columns_key = str(list(yellow_columns.keys())).replace(\",\", \";\")\n",
    "yellow_columns_value = str(list(yellow_columns.values())).replace(\",\", \";\")\n",
    "    \n",
    "# Define output after cleansing step\n",
    "cleansed_yellow_data = PipelineData(\"cleansed_yellow_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Cleanse script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# cleansing step creation\n",
    "# See the cleanse.py for details about input and output\n",
    "cleansingStepYellow = PythonScriptStep(\n",
    "    name=\"Cleanse Yellow Taxi Data\",\n",
    "    script_name=\"cleanse.py\", \n",
    "    arguments=[\"--useful_columns\", useful_columns,\n",
    "               \"--columns_key\", yellow_columns_key,\n",
    "                \"--columns_value\", yellow_columns_value,\n",
    "               \"--output_cleanse\", cleansed_yellow_data],\n",
    "    inputs=[yellow_taxi_data.as_named_input('raw_data')],\n",
    "    outputs=[cleansed_yellow_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"cleansingStepYellow created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge cleansed Green and Yellow datasets\n",
    "We are creating a single data source by merging the cleansed versions of Green and Yellow taxi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge script is in C:\\Users\\v-songshanli\\projects\\AML-Kubernetes\\docs\\ASH\\notebooks\\pipeline\\scripts\\prepdata.\n",
      "mergingStep created.\n"
     ]
    }
   ],
   "source": [
    "# Define output after merging step\n",
    "merged_data = PipelineData(\"merged_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Merge script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# merging step creation\n",
    "# See the merge.py for details about input and output\n",
    "mergingStep = PythonScriptStep(\n",
    "    name=\"Merge Taxi Data\",\n",
    "    script_name=\"merge.py\", \n",
    "    arguments=[\"--output_merge\", merged_data],\n",
    "    inputs=[cleansed_green_data.parse_parquet_files(),\n",
    "            cleansed_yellow_data.parse_parquet_files()],\n",
    "    outputs=[merged_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"mergingStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data\n",
    "This step filters out coordinates for locations that are outside the city border. We use a TypeConverter object to change the latitude and longitude fields to decimal type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter script is in C:\\Users\\v-songshanli\\projects\\AML-Kubernetes\\docs\\ASH\\notebooks\\pipeline\\scripts\\prepdata.\n",
      "FilterStep created.\n"
     ]
    }
   ],
   "source": [
    "# Define output after merging step\n",
    "filtered_data = PipelineData(\"filtered_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Filter script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# filter step creation\n",
    "# See the filter.py for details about input and output\n",
    "filterStep = PythonScriptStep(\n",
    "    name=\"Filter Taxi Data\",\n",
    "    script_name=\"filter.py\", \n",
    "    arguments=[\"--output_filter\", filtered_data],\n",
    "    inputs=[merged_data.parse_parquet_files()],\n",
    "    outputs=[filtered_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"FilterStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data\n",
    "In this step, we split the pickup and dropoff datetime values into the respective date and time columns and then we rename the columns to use meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize script is in C:\\Users\\v-songshanli\\projects\\AML-Kubernetes\\docs\\ASH\\notebooks\\pipeline\\scripts\\prepdata.\n",
      "normalizeStep created.\n"
     ]
    }
   ],
   "source": [
    "# Define output after normalize step\n",
    "normalized_data = PipelineData(\"normalized_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Normalize script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# normalize step creation\n",
    "# See the normalize.py for details about input and output\n",
    "normalizeStep = PythonScriptStep(\n",
    "    name=\"Normalize Taxi Data\",\n",
    "    script_name=\"normalize.py\", \n",
    "    arguments=[\"--output_normalize\", normalized_data],\n",
    "    inputs=[filtered_data.parse_parquet_files()],\n",
    "    outputs=[normalized_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"normalizeStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform data\n",
    "Transform the normalized taxi data to final required format. This steps does the following:\n",
    "\n",
    "- Split the pickup and dropoff date further into the day of the week, day of the month, and month values. \n",
    "- To get the day of the week value, uses the derive_column_by_example() function. The function takes an array parameter of example objects that define the input data, and the preferred output. The function automatically determines the preferred transformation. For the pickup and dropoff time columns, split the time into the hour, minute, and second by using the split_column_by_example() function with no example parameter.\n",
    "- After new features are generated, use the drop_columns() function to delete the original fields as the newly generated features are preferred. \n",
    "- Rename the rest of the fields to use meaningful descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform script is in C:\\Users\\v-songshanli\\projects\\AML-Kubernetes\\docs\\ASH\\notebooks\\pipeline\\scripts\\prepdata.\n",
      "transformStep created.\n"
     ]
    }
   ],
   "source": [
    "# Define output after transform step\n",
    "transformed_data = PipelineData(\"transformed_data\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Transform script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# transform step creation\n",
    "# See the transform.py for details about input and output\n",
    "transformStep = PythonScriptStep(\n",
    "    name=\"Transform Taxi Data\",\n",
    "    script_name=\"transform.py\", \n",
    "    arguments=[\"--output_transform\", transformed_data],\n",
    "    inputs=[normalized_data.parse_parquet_files()],\n",
    "    outputs=[transformed_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"transformStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test sets\n",
    "This function segregates the data into dataset for model training and dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data spilt script is in C:\\Users\\v-songshanli\\projects\\AML-Kubernetes\\docs\\ASH\\notebooks\\pipeline\\scripts\\trainmodel.\n",
      "testTrainSplitStep created.\n"
     ]
    }
   ],
   "source": [
    "train_model_folder = './scripts/trainmodel'\n",
    "\n",
    "# train and test splits output\n",
    "output_split_train = PipelineData(\"output_split_train\", datastore=default_store).as_dataset()\n",
    "output_split_test = PipelineData(\"output_split_test\", datastore=default_store).as_dataset()\n",
    "\n",
    "print('Data spilt script is in {}.'.format(os.path.realpath(train_model_folder)))\n",
    "\n",
    "# test train split step creation\n",
    "# See the train_test_split.py for details about input and output\n",
    "testTrainSplitStep = PythonScriptStep(\n",
    "    name=\"Train Test Data Split\",\n",
    "    script_name=\"train_test_split.py\", \n",
    "    arguments=[\"--output_split_train\", output_split_train,\n",
    "               \"--output_split_test\", output_split_test],\n",
    "    inputs=[transformed_data.parse_parquet_files()],\n",
    "    outputs=[output_split_train, output_split_test],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=train_model_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"testTrainSplitStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment created\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'NYCTaxi_Tutorial_Pipelines')\n",
    "\n",
    "print(\"Experiment created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a custom training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step created.\n"
     ]
    }
   ],
   "source": [
    "training_dataset = output_split_train.parse_parquet_files().keep_columns(['pickup_weekday','pickup_hour', 'distance','passengers', 'vendor', 'cost'])\n",
    "test_dataset = output_split_test.parse_parquet_files().keep_columns(['pickup_weekday', 'pickup_hour', 'distance', 'passengers', 'vendor', 'cost'])\n",
    "\n",
    "train_step = PythonScriptStep(\n",
    "        name=\"train_step\",\n",
    "        script_name=\"train_step.py\",\n",
    "        arguments=[],\n",
    "        inputs=[training_dataset, test_dataset],\n",
    "        outputs=[],\n",
    "        compute_target=aml_compute,\n",
    "        runconfig=aml_run_config,\n",
    "        source_directory=train_model_folder,\n",
    "        allow_reuse=True\n",
    "    )\n",
    "\n",
    "print(\"train_step created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and run the pipeline, register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n",
      "Created step train_step [5550c474][ec310479-8a37-46f8-8f4e-b37873d1af85], (This step is eligible to reuse a previous run's output)\n",
      "Created step Train Test Data Split [e4e16a0d][a0afea19-96b3-41ad-ba42-8e0aada3317a], (This step is eligible to reuse a previous run's output)\n",
      "Created step Transform Taxi Data [93eeec8e][e3ef1122-4c72-4d83-bdf8-2a10c5d9ca06], (This step is eligible to reuse a previous run's output)\n",
      "Created step Normalize Taxi Data [2b81ba51][17df0506-b087-453b-bcae-f09a91a44676], (This step is eligible to reuse a previous run's output)Created step Filter Taxi Data [1e77ffa7][593439b5-0c67-4148-a47f-baed6aee4601], (This step is eligible to reuse a previous run's output)\n",
      "Created step Merge Taxi Data [d3822db0][f2580a35-e901-41eb-8ce3-85e8bc6ba54a], (This step is eligible to reuse a previous run's output)\n",
      "\n",
      "Created step Cleanse Green Taxi Data [ccd0bfa2][3f15d738-4d66-4311-bfde-221f259d3f50], (This step is eligible to reuse a previous run's output)\n",
      "Created step Cleanse Yellow Taxi Data [2dcfa0de][77a22e3b-b790-44e6-8341-48d55f7f9334], (This step is eligible to reuse a previous run's output)\n",
      "Submitted PipelineRun a1b4d95a-c7d9-465a-9513-48b60898cf90\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/NYCTaxi_Tutorial_Pipelines/runs/a1b4d95a-c7d9-465a-9513-48b60898cf90?wsid=/subscriptions/6b736da6-3246-44dd-a0b8-b5e95484633d/resourcegroups/sl-ash2/workspaces/sl-ash2-mal\n",
      "Pipeline submitted for execution.\n",
      "PipelineRunId: a1b4d95a-c7d9-465a-9513-48b60898cf90\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/NYCTaxi_Tutorial_Pipelines/runs/a1b4d95a-c7d9-465a-9513-48b60898cf90?wsid=/subscriptions/6b736da6-3246-44dd-a0b8-b5e95484633d/resourcegroups/sl-ash2/workspaces/sl-ash2-mal\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': 'a1b4d95a-c7d9-465a-9513-48b60898cf90', 'status': 'Completed', 'startTimeUtc': '2021-02-27T00:08:29.264874Z', 'endTimeUtc': '2021-02-27T01:02:47.77685Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.a1b4d95a-c7d9-465a-9513-48b60898cf90/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=VzvtMfaLDrAoypDUKP8ANWkiDucuHold0Alci739C3A%3D&st=2021-02-27T00%3A52%3A48Z&se=2021-02-27T09%3A02%3A48Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.a1b4d95a-c7d9-465a-9513-48b60898cf90/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=vnytGa6dG15gstyuwkJX9LkGBuXA4NuW1EISbLmEigY%3D&st=2021-02-27T00%3A52%3A48Z&se=2021-02-27T09%3A02%3A48Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.a1b4d95a-c7d9-465a-9513-48b60898cf90/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=T2DL2Cb1amP092PcmiAZHBBfLp3Fq2w%2BEqiznyxfV%2Bo%3D&st=2021-02-27T00%3A52%3A48Z&se=2021-02-27T09%3A02%3A48Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='sl-ash2-mal', subscription_id='6b736da6-3246-44dd-a0b8-b5e95484633d', resource_group='sl-ash2'), name=taxi_model, id=taxi_model:8, version=8, tags={}, properties={})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "pipeline_steps = [train_step]\n",
    "\n",
    "pipeline = Pipeline(workspace = ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n",
    "\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "\n",
    "pipeline_run.wait_for_completion()\n",
    "\n",
    "train_step_run = pipeline_run.find_step_run(train_step.name)[0]\n",
    "train_step_run.register_model(model_name='taxi_model', model_path='outputs/taxi.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model named \"taxi_model\" should be registered in your AML workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Registered Model\n",
    "\n",
    "To test the trained model, you can create (or use existing) a AKS cluster for serving the model using AML deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Workspace, Model, ComputeTarget\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'aks-service-2'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    is_new_compute  = False\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws,\n",
    "                                    name = aks_name,\n",
    "                                    provisioning_configuration = prov_config)\n",
    "    is_new_compute  = True\n",
    "\n",
    "if aks_target.get_status() != \"Succeeded\":\n",
    "    aks_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './aml_src/conda-env.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-5cb1847acdb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m env = Environment.from_dockerfile(\n\u001b[0m\u001b[0;32m      2\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pytorch-obj-seg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mdockerfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./aml_src/Dockerfile.gpu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         conda_specification='./aml_src/conda-env.yaml')\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\azureml\\core\\environment.py\u001b[0m in \u001b[0;36mfrom_dockerfile\u001b[1;34m(name, dockerfile, conda_specification, pip_requirements)\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m         \"\"\"\n\u001b[1;32m--> 954\u001b[1;33m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconda_specification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpip_requirements\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         \u001b[1;31m# set base image to None to suppress warning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\azureml\\core\\environment.py\u001b[0m in \u001b[0;36m_from_dependencies\u001b[1;34m(name, conda_specification, pip_requirements)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconda_specification\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 928\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_conda_specification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconda_specification\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    929\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpip_requirements\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pip_requirements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpip_requirements\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\azureml\\core\\environment.py\u001b[0m in \u001b[0;36mfrom_conda_specification\u001b[1;34m(name, file_path)\u001b[0m\n\u001b[0;32m   1007\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \"\"\"  # noqa: E501\n\u001b[1;32m-> 1009\u001b[1;33m         \u001b[0mconda_dependencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCondaDependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconda_dependencies_file_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconda_dependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_version\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m             \u001b[0mmodule_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No Python version provided, defaulting to \"{}\"'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPYTHON_DEFAULT_VERSION\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pythonProject\\lib\\site-packages\\azureml\\core\\conda_dependencies.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, conda_dependencies_file_path, _underlying_structure)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;34m\"\"\"Initialize a new object to manage dependencies.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconda_dependencies_file_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconda_dependencies_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conda_dependencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mruamel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround_trip_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0m_underlying_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './aml_src/conda-env.yaml'"
     ]
    }
   ],
   "source": [
    "env = Environment.from_dockerfile(\n",
    "        name='pytorch-obj-seg',\n",
    "        dockerfile='./aml_src/Dockerfile.gpu',\n",
    "        conda_specification='./aml_src/conda-env.yaml')\n",
    "\n",
    "env.inferencing_stack_version='latest'\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
    "deploy_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "deployed_model = \"obj_seg_model_aml\" # model_name\n",
    "model = ws.models[deployed_model]\n",
    "\n",
    "service_name = 'objservice'\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=deploy_config,\n",
    "                       deployment_target=aks_target,\n",
    "                       overwrite=True)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model Using the Deployed Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the newly created cluster\n",
    "\n",
    "Note: This is important if you wish to avoid the cost of this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_new_compute:\n",
    "    aks_target.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Learn how to [download model then upload to Azure Storage blobs](../AML-model-download-upload.ipynb)\n",
    "2. Learn how to [inference using KFServing with model in Azure Storage Blobs](https://aka.ms/kfas)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "anshirga"
   }
  ],
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "pythonproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
