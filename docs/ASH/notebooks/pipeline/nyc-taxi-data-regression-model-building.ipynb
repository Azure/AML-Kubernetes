{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression for NYC Taxi Datasets with AzureML Pipelines \n",
    "\n",
    "In this notebook, you learn how to prepare data for regression modeling. You run various transformations to filter and combine two different NYC taxi datasets\n",
    "using [Azure Machine Learning Pipelines](https://aka.ms/aml-pipelines). After data process steps, you add a final custom training step to train a regression model.  The trained model then registered to AzureML workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "*    [ A Kubernetes cluster deployed on Azure Stack Hub, connected to Azure through ARC](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/AML-ARC-Compute.md).\n",
    "     \n",
    "\n",
    "*     [Datastore setup in Azure Machine Learning workspace backed up by Azure Stack Hub storage account](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md) \n",
    "\n",
    "\n",
    "*      Last but not least, you need to be able to run a Notebook. \n",
    "\n",
    "   If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize AzureML workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace,  ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data for regression modeling\n",
    "\n",
    "First, you will prepare data for regression modeling. you will leverage the convenience of Azure Open Datasets. Perform `pip install azureml-opendatasets` to get the open dataset package.  The Open Datasets package contains a class representing each data source (NycTlcGreen and NycTlcYellow) to easily filter date parameters before downloading.\n",
    "\n",
    "\n",
    "### Fetch data\n",
    "Begin by creating a dataframe to hold the taxi data. When working in a non-Spark environment, Open Datasets only allows downloading one month of data at a time with certain classes to avoid MemoryError with large datasets. To download a year of taxi data, iteratively fetch one month at a time, and before appending it to green_df_raw, randomly sample 500 records from each month to avoid bloating the dataframe. Then preview the data. To keep this process short, we are sampling data of only 1 month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.opendatasets import NycTlcGreen, NycTlcYellow\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "green_df_raw = pd.DataFrame([])\n",
    "start = datetime.strptime(\"1/1/2016\",\"%m/%d/%Y\")\n",
    "end = datetime.strptime(\"1/31/2016\",\"%m/%d/%Y\")\n",
    "\n",
    "number_of_months = 1\n",
    "sample_size = 5000\n",
    "\n",
    "for sample_month in range(number_of_months):\n",
    "    temp_df_green = NycTlcGreen(start + relativedelta(months=sample_month), end + relativedelta(months=sample_month)) \\\n",
    "        .to_pandas_dataframe()\n",
    "    green_df_raw = green_df_raw.append(temp_df_green.sample(sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_df_raw = pd.DataFrame([])\n",
    "start = datetime.strptime(\"1/1/2016\",\"%m/%d/%Y\")\n",
    "end = datetime.strptime(\"1/31/2016\",\"%m/%d/%Y\")\n",
    "\n",
    "sample_size = 500\n",
    "\n",
    "for sample_month in range(number_of_months):\n",
    "    temp_df_yellow = NycTlcYellow(start + relativedelta(months=sample_month), end + relativedelta(months=sample_month)) \\\n",
    "        .to_pandas_dataframe()\n",
    "    yellow_df_raw = yellow_df_raw.append(temp_df_yellow.sample(sample_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(green_df_raw.head(5))\n",
    "display(yellow_df_raw.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataDir = \"data\"\n",
    "\n",
    "if not os.path.exists(dataDir):\n",
    "    os.mkdir(dataDir)\n",
    "\n",
    "greenDir = dataDir + \"/green\"\n",
    "yelloDir = dataDir + \"/yellow\"\n",
    "\n",
    "if not os.path.exists(greenDir):\n",
    "    os.mkdir(greenDir)\n",
    "    \n",
    "if not os.path.exists(yelloDir):\n",
    "    os.mkdir(yelloDir)\n",
    "    \n",
    "greenTaxiData = greenDir + \"/unprepared.csv\"\n",
    "yellowTaxiData = yelloDir + \"/unprepared.csv\"\n",
    "\n",
    "green_df_raw.to_csv(greenTaxiData, index=False)\n",
    "yellow_df_raw.to_csv(yellowTaxiData, index=False)\n",
    "\n",
    "print(\"Data written to local folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "Your next step is to upload these files to datastore of the workspace, and then registered as dataset in the workspace. \n",
    "  \n",
    "\"datastore_name\" is the name of the datastore you setup in [this step](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md).\n",
    "\n",
    "Upload and dataset registration take less than 1 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "\n",
    "datastore_name = \"<NAME_OF_ASH_HOSTED_AML_DATASTORE>\"\n",
    "datastore =  Datastore.get(ws, datastore_name)\n",
    "\n",
    "dataDir = \"data\"\n",
    "greenDir, yelloDir  = dataDir + \"/green\", dataDir + \"/yellow\"\n",
    "greenTaxiData, yellowTaxiData = greenDir + \"/unprepared.csv\", yelloDir + \"/unprepared.csv\"\n",
    "\n",
    "datastore.upload_files([greenTaxiData], \n",
    "                           target_path = 'green', \n",
    "                           overwrite = True, \n",
    "                           show_progress = True)\n",
    "\n",
    "datastore.upload_files([yellowTaxiData], \n",
    "                           target_path = 'yellow', \n",
    "                           overwrite = True, \n",
    "                           show_progress = True)\n",
    "\n",
    "print(\"Upload calls completed.\")\n",
    "\n",
    "# register datasets\n",
    "\n",
    "dataset_name,  target_path = 'green_taxi_data_o_file', 'green/unprepared.csv'\n",
    "datastore_paths = [(datastore, target_path)]\n",
    "green_taxi_data = Dataset.File.from_files(path=datastore_paths)\n",
    "green_taxi_data.register(ws, dataset_name)\n",
    "\n",
    "dataset_name,  target_path = 'yellow_taxi_data_o_file', 'yellow/unprepared.csv'\n",
    "datastore_paths = [(datastore, target_path)]\n",
    "yellow_taxi_data = Dataset.File.from_files(path=datastore_paths)\n",
    "yellow_taxi_data.register(ws, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup compute target\n",
    "\n",
    "Find the attach name for the Arc enabled  Azure Stack Hub kubernetes cluster in your AzureML workspace to create a ComputeTarget:\n",
    "\n",
    "attach_name is the attached name for your ASH cluster you setup in [this step](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/AML-ARC-Compute.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import KubernetesCompute\n",
    "\n",
    "attach_name = \"<NAME_OF_AML_ATTACHED_COMPUTE_OF_YOUR_ASH_CLUSTER>\"\n",
    "arcK_compute = KubernetesCompute(ws, attach_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RunConfig for the compute target\n",
    "We will also use `pandas`, `scikit-learn`,  `pyarrow` for the pipeline steps. Defining the `runconfig` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a new runconfig object\n",
    "aml_run_config = RunConfiguration()\n",
    "\n",
    "# Use the arcK_compute you created above. \n",
    "aml_run_config.target = arcK_compute\n",
    "\n",
    "# Enable Docker\n",
    "aml_run_config.environment.docker.enabled = True\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=['pandas','scikit-learn'], \n",
    "    pip_packages=['azureml-sdk', 'pyarrow'])\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data with pipeline steps\n",
    "\n",
    "The data process includes following pipeline steps implemented as PythonScriptStep:\n",
    "\n",
    "1. Cleanse Green Taxi Data\n",
    "2. Cleanse Yellow Taxi Data\n",
    "3. Merge cleansed Green and Yellow Taxi Data\n",
    "4. Filter Taxi Data\n",
    "5. Normalize Taxi Data\n",
    "6. Transform Taxi Data\n",
    "7. Train Test Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse Taxi Data\n",
    "\n",
    "Here a set of \"useful\" columns for both Green and Yellow taxi data are defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_columns = str([\"cost\", \"distance\", \"dropoff_datetime\", \"dropoff_latitude\", \n",
    "                      \"dropoff_longitude\", \"passengers\", \"pickup_datetime\", \n",
    "                      \"pickup_latitude\", \"pickup_longitude\", \"store_forward\", \"vendor\"]).replace(\",\", \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanse Green taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "import os\n",
    "\n",
    "# python scripts folder\n",
    "prepare_data_folder = './scripts/prepdata'\n",
    "\n",
    "# rename columns as per Azure Machine Learning NYC Taxi tutorial\n",
    "\n",
    "green_columns = {\n",
    "        \"vendorID\": \"vendor\",\n",
    "        \"lpepPickupDatetime\": \"pickup_datetime\",\n",
    "        \"lpepDropoffDatetime\": \"dropoff_datetime\",\n",
    "        \"storeAndFwdFlag\": \"store_forward\",\n",
    "        \"pickupLongitude\": \"pickup_longitude\",\n",
    "        \"pickupLatitude\": \"pickup_latitude\",\n",
    "        \"dropoffLongitude\": \"dropoff_longitude\",\n",
    "        \"dropoffLatitude\": \"dropoff_latitude\",\n",
    "        \"passengerCount\": \"passengers\",\n",
    "        \"fareAmount\": \"cost\",\n",
    "        \"tripDistance\": \"distance\"\n",
    "    }\n",
    "\n",
    "green_columns_key = str(list(green_columns.keys())).replace(\",\", \";\")\n",
    "green_columns_value = str(list(green_columns.values())).replace(\",\", \";\")\n",
    "    \n",
    "# Define output after cleansing step\n",
    "dest = (datastore, None)\n",
    "cleansed_green_data = OutputFileDatasetConfig(name=\"cleansed_green_data\", destination=dest).as_upload(overwrite=False)\n",
    "\n",
    "print('Cleanse script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# cleansing step creation\n",
    "# See the cleanse.py for details\n",
    "cleansingStepGreen = PythonScriptStep(\n",
    "    name=\"Cleanse Green Taxi Data\",\n",
    "    script_name=\"cleanse.py\", \n",
    "    arguments=[\"--data-path\", green_taxi_data.as_named_input('raw_data').as_mount(),\n",
    "        \"--useful_columns\", useful_columns,\n",
    "               \"--columns_key\", green_columns_key,\n",
    "                \"--columns_value\", green_columns_value,\n",
    "               \"--output_cleanse\", cleansed_green_data],\n",
    "    outputs=[cleansed_green_data],\n",
    "    compute_target=arcK_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"cleansingStepGreen created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanse Yellow taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_columns = {\n",
    "    \"vendorID\": \"vendor\",\n",
    "    \"tpepPickupDateTime\": \"pickup_datetime\",\n",
    "    \"tpepDropoffDateTime\": \"dropoff_datetime\",\n",
    "    \"storeAndFwdFlag\": \"store_forward\",\n",
    "    \"startLon\": \"pickup_longitude\",\n",
    "    \"startLat\": \"pickup_latitude\",\n",
    "    \"endLon\": \"dropoff_longitude\",\n",
    "    \"endLat\": \"dropoff_latitude\",\n",
    "    \"passengerCount\": \"passengers\",\n",
    "    \"fareAmount\": \"cost\",\n",
    "    \"tripDistance\": \"distance\"\n",
    "}\n",
    "\n",
    "yellow_columns_key = str(list(yellow_columns.keys())).replace(\",\", \";\")\n",
    "yellow_columns_value = str(list(yellow_columns.values())).replace(\",\", \";\")\n",
    "    \n",
    "# Define output after cleansing step\n",
    "dest = (datastore, None)\n",
    "cleansed_yellow_data = OutputFileDatasetConfig(name=\"cleansed_yellow_data\", destination=dest).as_upload(overwrite=False)\n",
    "\n",
    "# cleansing step creation\n",
    "# See the cleanse.py for details about input and output\n",
    "cleansingStepYellow = PythonScriptStep(\n",
    "    name=\"Cleanse Yellow Taxi Data\",\n",
    "    script_name=\"cleanse.py\", \n",
    "    arguments=[\"--data-path\", yellow_taxi_data.as_named_input('raw_data').as_mount(),\n",
    "        \"--useful_columns\", useful_columns,\n",
    "               \"--columns_key\", yellow_columns_key,\n",
    "                \"--columns_value\", yellow_columns_value,\n",
    "               \"--output_cleanse\", cleansed_yellow_data],\n",
    "    compute_target=arcK_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"cleansingStepYellow created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge cleansed Green and Yellow datasets\n",
    "\n",
    "Create a single data source by merging the cleansed versions of Green and Yellow taxi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output after cleansing step\n",
    "merged_data = OutputFileDatasetConfig(name=\"merged_data\", destination=dest).as_upload(overwrite=False)\n",
    "\n",
    "\n",
    "print('Merge script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# merging step creation\n",
    "# See the merge.py for details about input and output\n",
    "mergingStep = PythonScriptStep(\n",
    "    name=\"Merge Taxi Data\",\n",
    "    script_name=\"merge.py\", \n",
    "    arguments=[\"--green_data_path\", cleansed_green_data.as_input(),\n",
    "               \"--yellow_data_path\", cleansed_yellow_data.as_input(),\n",
    "        \"--output_merge\", merged_data],\n",
    "    compute_target=arcK_compute,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"mergingStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter taxi data\n",
    "\n",
    "This step filters out coordinates for locations that are outside the city border. We use a TypeConverter object to change the latitude and longitude fields to decimal type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output after merging step\n",
    "filtered_data = OutputFileDatasetConfig(name=\"filtered_data\", destination=dest).as_upload(overwrite=False)\n",
    "\n",
    "print('Filter script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# filter step creation\n",
    "# See the filter.py for details about input and output\n",
    "filterStep = PythonScriptStep(\n",
    "    name=\"Filter Taxi Data\",\n",
    "    script_name=\"filter.py\", \n",
    "    arguments=[\"--data-path\", merged_data.as_input(),\n",
    "        \"--output_filter\", filtered_data],\n",
    "    compute_target=arcK_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"FilterStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize taxi data\n",
    "In this step, pickup and dropoff datetime values are splitted into the respective date and time columns.  Then rename the columns to meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output after normalize step\n",
    "normalized_data = OutputFileDatasetConfig(name=\"normalized_data\", destination=dest).as_upload(overwrite=False)\n",
    "\n",
    "print('Normalize script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# normalize step creation\n",
    "# See the normalize.py for details about input and output\n",
    "normalizeStep = PythonScriptStep(\n",
    "    name=\"Normalize Taxi Data\",\n",
    "    script_name=\"normalize.py\", \n",
    "    arguments=[\"--data-path\", filtered_data.as_input(),\n",
    "        \"--output_normalize\", normalized_data],\n",
    "    compute_target=arcK_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"normalizeStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform taxi data\n",
    "Transform the normalized taxi data to final required format. This steps does the following:\n",
    "\n",
    "- Split the pickup and dropoff date further into the day of the week, day of the month, and month values. \n",
    "- To get the day of the week value, uses the derive_column_by_example() function. The function takes an array parameter of example objects that define the input data, and the preferred output. The function automatically determines the preferred transformation. For the pickup and dropoff time columns, split the time into the hour, minute, and second by using the split_column_by_example() function with no example parameter.\n",
    "- After new features are generated, use the drop_columns() function to delete the original fields as the newly generated features are preferred. \n",
    "- Rename the rest of the fields to use meaningful descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output after transform step\n",
    "transformed_data = OutputFileDatasetConfig(name=\"transformed_data\", destination=dest).as_upload(overwrite=False)\n",
    "\n",
    "print('Transform script is in {}.'.format(os.path.realpath(prepare_data_folder)))\n",
    "\n",
    "# transform step creation\n",
    "# See the transform.py for details about input and output\n",
    "transformStep = PythonScriptStep(\n",
    "    name=\"Transform Taxi Data\",\n",
    "    script_name=\"transform.py\", \n",
    "    arguments=[\"--data-path\", normalized_data.as_input(),\n",
    "        \"--output_transform\", transformed_data],\n",
    "    compute_target=arcK_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=prepare_data_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"transformStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test sets\n",
    "\n",
    "This function segregates the data into dataset for model training and dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_folder = './scripts/trainmodel'\n",
    "\n",
    "# train and test splits output\n",
    "output_split_train = OutputFileDatasetConfig(name=\"output_split_train\", destination=dest).as_upload(overwrite=False)\n",
    "output_split_test = OutputFileDatasetConfig(name=\"output_split_test\", destination=dest).as_upload(overwrite=False)\n",
    "\n",
    "output_split_test.register_on_complete(name='nyc_taxi_test_set', \n",
    "                                       description='test set  from Train Test Data Split')\n",
    "\n",
    "print('Data spilt script is in {}.'.format(os.path.realpath(train_model_folder)))\n",
    "\n",
    "# test train split step creation\n",
    "# See the train_test_split.py for details about input and output\n",
    "testTrainSplitStep = PythonScriptStep(\n",
    "    name=\"Train Test Data Split\",\n",
    "    script_name=\"train_test_split.py\", \n",
    "    arguments=[\"--data-path\", transformed_data.as_input(),\n",
    "        \"--output_split_train\", output_split_train,\n",
    "               \"--output_split_test\", output_split_test],\n",
    "    compute_target=arcK_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=train_model_folder,\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print(\"testTrainSplitStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a custom training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(\n",
    "        name=\"train_step\",\n",
    "        script_name=\"train_step.py\",\n",
    "        arguments=[\"--train_data_path\", output_split_train.as_input(),\n",
    "                   \"--test_data_path\", output_split_test.as_input(),\n",
    "        ],\n",
    "      \n",
    "        compute_target=arcK_compute,\n",
    "        runconfig=aml_run_config,\n",
    "        source_directory=train_model_folder,\n",
    "        allow_reuse=True\n",
    "    )\n",
    "\n",
    "print(\"train_step created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'NYCTaxi_Tutorial_Pipelines')\n",
    "\n",
    "print(\"Experiment created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline_steps = [train_step]\n",
    "\n",
    "pipeline = Pipeline(workspace = ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n",
    "\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model\n",
    "\n",
    "Register the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_run = pipeline_run.find_step_run(train_step.name)[0]\n",
    "\n",
    "registered_model_name='taxi_model'\n",
    "train_step_run.register_model(model_name=registered_model_name, model_path='outputs/taxi.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model named \"taxi_model\" should be registered in your AML workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Registered Model\n",
    "\n",
    "To test the trained model, you can create (or use existing) a AKS cluster for serving the model using AML deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Workspace, Model, ComputeTarget\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision the AKS Cluster\n",
    "\n",
    "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it. It may take 5 mins to create a new AKS cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'aks-service-2'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    is_new_compute  = False\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws,\n",
    "                                    name = aks_name,\n",
    "                                    provisioning_configuration = prov_config)\n",
    "    is_new_compute  = True\n",
    "\n",
    "if aks_target.get_status() != \"Succeeded\":\n",
    "    aks_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "# Enable Docker\n",
    "env  = Environment(\"nyc taxi\")\n",
    "env.docker.enabled = True\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "env.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "env.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=['pandas','scikit-learn','numpy==1.19.5'], \n",
    "    pip_packages=['azureml-defaults', 'pyarrow'])\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
    "deploy_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "deployed_model =  registered_model_name \n",
    "model = ws.models[deployed_model]\n",
    "\n",
    "service_name = 'nyctaxiservice1'\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=deploy_config,\n",
    "                       deployment_target=aks_target,\n",
    "                       overwrite=True)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with inputs\n",
    "\n",
    "A few intances of test data are stored in test_set.csv for convenience of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_csv(\"test_set.csv\")\n",
    "selected_columns = ['pickup_weekday', 'pickup_hour', 'distance', 'passengers', 'vendor']\n",
    "df = df_raw[selected_columns][:3]\n",
    "input_np = df.to_numpy()\n",
    "\n",
    "instances = json.dumps({\"data\": input_np.tolist()})\n",
    "\n",
    "predicted_costs = service.run(instances)\n",
    "\n",
    "print(predicted_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real costs\n",
    "\n",
    "df_raw[\"cost\"][:3].to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the newly created cluster\n",
    "\n",
    "Note: This is important if you wish to avoid the cost of this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_new_compute:\n",
    "    aks_target.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Learn how to [download model then upload to Azure Storage blobs](../AML-model-download-upload.ipynb)\n",
    "2. Learn how to [inference using KFServing with model in Azure Storage Blobs](https://aka.ms/kfas)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "anshirga"
   }
  ],
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "pythonproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}