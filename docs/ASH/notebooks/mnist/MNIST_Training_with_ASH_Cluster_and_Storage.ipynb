{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Training on ASH cluster and storage account\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "*     A Kubernetes cluster deployed on Azure Stack Hub, connected to Azure through ARC.\n",
    "     \n",
    "   For details on how to deploy kubernetes cluster on Azure Stack Hub and enabling ARC connection to Azure, please follow [this guide](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/AML-ARC-Compute.md)\n",
    "  \n",
    "\n",
    "*     Datastore setup in Azure Machine Learning workspace backed up by Azure Stack Hub storage account.\n",
    "\n",
    "   [This document](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md) is a detailed guide on how to create Azure Machine Learning workspace, create a  Azure Stack Hub Storage account, and setup datastore in AML workspace backed by ASH storage account.\n",
    "\n",
    "\n",
    "*      Last but not least, you need to be able to run a Notebook. \n",
    "\n",
    "   If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace,  ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "You may download mnist data from [mnist_data](http://yann.lecun.com/exdb/mnist/). For your convenience, we have included under mnist_data folder. There are four files: t10k-images-idx3-ubyte.gz, t10k-labels-idx1-ubyte.gz, train-images-idx3-ubyte.gz and train-labels-idx1-ubyte.gz.  Your next step is to upload these files to datastore of the workspace, and then registered as dataset in the workspace. \n",
    "\n",
    "Upload and dataset registration take less than 1 min.\n",
    "\n",
    "To set up datastore using an azure stack hub storage account, please refer to [Train_azure_arc](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md). To register the dataset manually, please refer to this [video](https://msit.microsoftstream.com/video/51f7a3ff-0400-b9eb-2703-f1eb38bc6232)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "\n",
    "dataset_name = \"mnist_ash\"\n",
    "if dataset_name not  in ws.datasets:\n",
    "    \n",
    "    datastore_name = \"ashstore\"\n",
    "    datastore =  Datastore.get(ws, datastore_name)\n",
    "    \n",
    "    src_dir, target_path = 'mnist_data', 'mnistdataash'\n",
    "    datastore.upload(src_dir, target_path)\n",
    "\n",
    "    # register data uploaded as AML dataset\n",
    "    datastore_paths = [(datastore, target_path)]\n",
    "    pd_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "    pd_ds.register(ws, dataset_name, \"mnist data from http://yann.lecun.com/exdb/mnist/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or attach existing ArcKubernetesCompute\n",
    "\n",
    "The attaching code here depends  python package azureml-contrib-k8s which current is in private preview. Install private preview branch of AzureML SDK by running following command (private preview):\n",
    "\n",
    "<pre>\n",
    "pip install --disable-pip-version-check --extra-index-url https://azuremlsdktestpypi.azureedge.net/azureml-contrib-k8s-preview/D58E86006C65 azureml-contrib-k8s\n",
    "</pre>\n",
    "\n",
    "Attaching ASH cluster the first time may take 7 minutes. It will be much faster after first attachment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.core.compute.arckubernetescompute import ArcKubernetesCompute\n",
    "\n",
    "resource_id = \"<resource_id>\"\n",
    "\n",
    "attach_config = ArcKubernetesCompute.attach_configuration(\n",
    "    resource_id= resource_id,\n",
    ")\n",
    "\n",
    "try:\n",
    "    attach_name = \"arcattached\"\n",
    "    arcK_target_result = ArcKubernetesCompute.attach(ws, attach_name, attach_config)\n",
    "    arcK_target_result.wait_for_completion(show_output=True)\n",
    "    print('arc attach  success')\n",
    "except ComputeTargetException as e:\n",
    "    print(e)\n",
    "    print('arc attach  failed')\n",
    "\n",
    "arcK_target = ws.compute_targets[attach_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the training job and Submit a run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'mnist-demo'\n",
    "\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# to install required packages\n",
    "env = Environment('tutorial-env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults'], conda_packages = ['scikit-learn==0.22.1'])\n",
    "\n",
    "env.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "The training takes about 15 mins with vm size comparable  to Standard_DS3_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "args = ['--data-folder', ws.datasets[dataset_name].as_mount(), '--regularization', 0.5]\n",
    "script_folder =  \"mnist_script\"\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='train.py', \n",
    "                      arguments=args,\n",
    "                      compute_target=arcK_target,\n",
    "                      environment=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=src)\n",
    "run.wait_for_completion(show_output=True)  # specify True for a verbose log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register model\n",
    "model = run.register_model(model_name='sklearn_mnist',\n",
    "                           model_path='outputs/sklearn_mnist_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Workspace, Model, ComputeTarget\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'aks-service-2'\n",
    "\n",
    "if aks_name not in  ws.compute_targets:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws,\n",
    "                                    name = aks_name,\n",
    "                                    provisioning_configuration = prov_config)\n",
    "    is_new_compute  = True\n",
    "\n",
    "    if aks_target.get_status() != \"Succeeded\":\n",
    "        aks_target.wait_for_completion(show_output=True)\n",
    "else:  \n",
    "    aks_target =  ws.compute_targets[aks_name]   \n",
    "    is_new_compute  = False\n",
    "    \n",
    "print(\"using compute target: \", aks_target.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "model = Model(ws, 'sklearn_mnist')\n",
    "\n",
    "\n",
    "env = Environment('tutorial-env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults'], conda_packages = ['scikit-learn==0.22.1'])\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)\n",
    "deploy_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                       name='sklearn-mnist-svc', \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_target=aks_target,\n",
    "                       deployment_config=deploy_config,\n",
    "                       overwrite=True)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data\n",
    "\n",
    "Load the test data from the ./data/ directory created during the training tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_script.utils import load_data\n",
    "import os\n",
    "import glob\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'mnist_data')\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster\n",
    "X_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\n",
    "y_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\n",
    "\n",
    "import json\n",
    "test = json.dumps({\"data\": X_test.tolist()})\n",
    "test = bytes(test, encoding='utf8')\n",
    "y_hat = service.run(input_data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mx = confusion_matrix(y_test, y_hat)\n",
    "print(conf_mx)\n",
    "print('Overall accuracy:', np.average(y_hat == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the diagonal cells so that they don't overpower the rest of the cells when visualized\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(norm_conf_mx, cmap=plt.cm.bone)\n",
    "ticks = np.arange(0, 10, 1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(ticks)\n",
    "ax.set_yticklabels(ticks)\n",
    "fig.colorbar(cax)\n",
    "plt.ylabel('true labels', fontsize=14)\n",
    "plt.xlabel('predicted values', fontsize=14)\n",
    "plt.savefig('confusion.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show predictions\n",
    "\n",
    "Test the deployed model with a random sample of 30 images from the test data.\n",
    "\n",
    "Print the returned predictions and plot them along with the input images. Red font and inverse image (white on black) is used to highlight the misclassified samples.\n",
    "Since the model accuracy is high, you might have to run the following code a few times before you can see a misclassified sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# find 30 random samples from test set\n",
    "n = 30\n",
    "sample_indices = np.random.permutation(X_test.shape[0])[0:n]\n",
    "\n",
    "test_samples = json.dumps({\"data\": X_test[sample_indices].tolist()})\n",
    "test_samples = bytes(test_samples, encoding='utf8')\n",
    "\n",
    "# predict using the deployed model\n",
    "result = service.run(input_data=test_samples)\n",
    "\n",
    "# compare actual value vs. the predicted values:\n",
    "i = 0\n",
    "plt.figure(figsize = (20, 1))\n",
    "\n",
    "for s in sample_indices:\n",
    "    plt.subplot(1, n, i + 1)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    \n",
    "    # use different color for misclassified sample\n",
    "    font_color = 'red' if y_test[s] != result[i] else 'black'\n",
    "    clr_map = plt.cm.gray if y_test[s] != result[i] else plt.cm.Greys\n",
    "    \n",
    "    plt.text(x=10, y=-10, s=result[i], fontsize=18, color=font_color)\n",
    "    plt.imshow(X_test[s].reshape(28, 28), cmap=clr_map)\n",
    "    \n",
    "    i = i + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the newly created cluster\n",
    "\n",
    "Note: This is important if you wish to avoid the cost of this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_new_compute:\n",
    "    aks_target.delete()\n",
    "    service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Learn how to [distributed training with pytorch](../distributed-cifar10/distributed-pytorch-cifar10.ipynb)\n",
    "2. Learn how to [distributed training with tensorflow](../distributed-cifar10/distributed-tf2-cifar10.ipynb)\n",
    "3. Learn Pipeline Steps with [Object Segmentation](../object-segmentation-on-azure-stack/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "pythonproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
