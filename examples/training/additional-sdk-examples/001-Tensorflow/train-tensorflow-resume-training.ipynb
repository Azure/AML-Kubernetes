{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Tensorflow training \n",
        "In this tutorial, you will train a mnist model in TensorFlow."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning (AML)\n",
        "* Go through the prerequisites in the [Azure Machine Learning documentation](https://docs.microsoft.com/azure/machine-learning/service/tutorial-train-models-with-aml#prerequisites):\n",
        "    * install the AML SDK\n",
        "    * create a workspace and its configuration file (`config.json`)\n",
        "[//]: # * Review the [tutorial](../train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb) on single-node TensorFlow training using the SDK"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1605464300648
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize workspace\n",
        "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.workspace import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep='\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1605468991404
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specify existing Kubernetes Compute"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, KubernetesCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# choose a name for your Kubernetes compute\n",
        "compute_name = 'gpucluster-1x'\n",
        "compute_target = ComputeTarget(workspace=ws, name=compute_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1605469001539
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_target"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1605469003539
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Dataset for Files\n",
        "A Dataset can reference single or multiple files in your datastores or public urls. The files can be of any format. Dataset provides you with the ability to download or mount the files to your compute. By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred. [Learn More](https://aka.ms/azureml/howto/createdatasets)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize file dataset \n",
        "from azureml.core.dataset import Dataset\n",
        "web_paths = ['http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
        "             'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
        "             'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
        "             'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
        "            ]\n",
        "dataset = Dataset.File.from_files(path = web_paths)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1605464337719
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "you may want to register datasets using the register() method to your workspace so they can be shared with others, reused across various experiments, and referred to by name in your training script."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#register dataset to workspace\n",
        "dataset = dataset.register(workspace = ws,\n",
        "                           name = 'mnist dataset for Arc',\n",
        "                           description='training and test dataset',\n",
        "                           create_new_version=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1605464356224
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list the files referenced by dataset\n",
        "dataset"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1605464366363
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model on the Kubernetes compute"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a project directory\n",
        "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "script_folder = './tf-resume-training'\n",
        "os.makedirs(script_folder, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1605464372057
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the training script `tf_mnist_with_checkpoint.py` into this project directory."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# the training logic is in the tf_mnist_with_checkpoint.py file.\n",
        "shutil.copy('./tf_mnist_with_checkpoint.py', script_folder)\n",
        "\n",
        "shutil.copy('./utils.py', script_folder)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1605465394555
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an experiment\n",
        "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this distributed TensorFlow tutorial. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "experiment_name = 'akse-arc-tf-training1'\n",
        "experiment = Experiment(ws, name=experiment_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1605469036186
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create ScriptRun"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.environment import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "env = Environment(\"env-tf\")\n",
        "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults','tensorflow==1.13.1','horovod==0.16.1'])\n",
        "env.docker.base_image='mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04'\n",
        "env.python.conda_dependencies = cd\n",
        "\n",
        "# Register environment to re-use later\n",
        "env.register(workspace = ws)\n",
        "\n",
        "from azureml.core import ScriptRunConfig\n",
        "args=['--data-folder', dataset.as_named_input('mnist').as_mount()]\n",
        "src = ScriptRunConfig(source_directory=script_folder,\n",
        "                      script='tf_mnist_with_checkpoint.py',\n",
        "                      compute_target=compute_target,\n",
        "                      environment=env,\n",
        "                      arguments=args)                     "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1605469045449
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we passed our training data reference `ds_data` to our script's `--data-folder` argument. This will 1) mount our datastore on the remote compute and 2) provide the path to the data zip file on our datastore."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit job\n",
        "### Run your experiment . Note that this call is asynchronous."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run = experiment.submit(src)\n",
        "print(run)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1605469051600
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitor your run\n",
        "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "index_order": 1,
    "exclude_from_index": false,
    "task": "Resume a model in TensorFlow from a previously submitted run",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "hesuri"
      }
    ],
    "kernel_info": {
      "name": "python3-azureml"
    },
    "msauthor": "hesuri",
    "language_info": {
      "name": "python",
      "version": "3.6.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "AML Compute"
    ],
    "kernelspec": {
      "name": "python3613jvsc74a57bd0f8e18c978b0aad5c358277e0d895e7dda4954c3f3c8dadc6287d53f0de7a9ea9",
      "display_name": "Python 3.6.13 64-bit ('amlarc': conda)"
    },
    "tags": [
      "None"
    ],
    "datasets": [
      "MNIST"
    ],
    "category": "training",
    "framework": [
      "TensorFlow"
    ],
    "friendly_name": "Resuming a model",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
